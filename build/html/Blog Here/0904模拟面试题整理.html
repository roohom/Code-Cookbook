

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>1. 面试题整理 &mdash; Code-Cookbook 0.1 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="2. 实时存储NoSQL面试" href="1008%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86.html" />
    <link rel="prev" title="Blogs" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Code-Cookbook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">大数据</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bigdata%20Tools/index.html">Bigdata Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">博客</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Blogs</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">1. 面试题整理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hadoopsplitblock">1.1. 1.Hadoop中Split和Block的区别？(看这篇博客)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hive">1.2. 2.Hive中如何处理小文件？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">1.3. 3.推测执行机制？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hadoop-java-java-object-serialization">1.4. 4.Hadoop为什么要自己实现序列化，而不用Java自带的序列化？（为什么不用Java Object Serialization？）</a></li>
<li class="toctree-l3"><a class="reference internal" href="#yarn">1.5. 5.yarn的三种调度策略?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#blocksize">1.6. 6.blocksize是根据什么设置?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">1.7. 7.分布式缓存?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">1.8. 8.blocksize是根据什么设置?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hivelock">1.9. 9.hive支持lock吗?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hdfsblock">1.10. 10.为什么HDFS的Block不能设置太大也不能设置太小？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hdfs-block-128m">1.11. 11.HDFS中块（block）的大小为什么设置为128M？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">1.12. 12.Hive严格模式和非严格模式的区别？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#jvm">1.13. 13.JVM的重用机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">1.14. 14.Hive文件存储格式及他们的类型(基于行还是列)？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">1.15. 15.分区与分桶的区别？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">1.16. 16.Yarn的产生是为了解决什么问题？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hiveorder-bysort-bydistribute-bycluster-by">1.17. 17.Hive中order by、sort by、distribute by和cluster by的区别和联系？</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="1008%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86.html">2. 实时存储NoSQL面试</a></li>
<li class="toctree-l2"><a class="reference internal" href="Annotation.html">3. 元注解</a></li>
<li class="toctree-l2"><a class="reference internal" href="Annotation.html#id2">4. 注解解析</a></li>
<li class="toctree-l2"><a class="reference internal" href="Closure%26Currying.html">5. Scala函数中闭包(Closure)和柯里化(Currying)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Collection.html">6. 集合</a></li>
<li class="toctree-l2"><a class="reference internal" href="Collection.html#id9">7. 简单(常用)数据结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="Commands.html">8. Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="EOF.html">9. EOF</a></li>
<li class="toctree-l2"><a class="reference internal" href="Git.html">10. Git问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hadoop%20distcp.html">11. Hadoop distcp</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hive%20SQL50%E9%A2%98%E8%AE%B0%E5%BD%95.html">12. Hive SQL50题记录</a></li>
<li class="toctree-l2"><a class="reference internal" href="IO%E6%B5%81.html">13. <code class="docutils literal notranslate"><span class="pre">IO</span> <span class="pre">Stream</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="Java%20Cookbook.html">14. JAVA Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java8%20Stream%28%29.html">15. Java8 Stream API</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java_Maven.html">16. Maven</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java_OOP.html">17. Java_OOP</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java%E4%B8%ADSocket%E5%8F%91%E9%80%81%E6%96%87%E4%BB%B6%E8%87%B3%E6%9C%8D%E5%8A%A1%E7%AB%AF.html">18. 使用Java在服务端和客户端之间传送文件</a></li>
<li class="toctree-l2"><a class="reference internal" href="PySpark.html">19. PySpark</a></li>
<li class="toctree-l2"><a class="reference internal" href="PySparkOnYarn.html">20. PySpark On Yarn</a></li>
<li class="toctree-l2"><a class="reference internal" href="SimpleDataStruct.html">21. 简单(常用)数据结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="Socket.html">22. Socket</a></li>
<li class="toctree-l2"><a class="reference internal" href="Spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1RSA%20premaster%20secret%20error.html">23. Spark提交任务RSA premaster secret error</a></li>
<li class="toctree-l2"><a class="reference internal" href="Springboot%E6%95%B4%E5%90%88Spark%2C%20%E6%9C%AC%E5%9C%B0%E3%80%81%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html">24. Springboot整合Spark, 本地、集群部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="Vim%E6%9F%A5%E6%89%BE%E5%92%8C%E6%9B%BF%E6%8D%A2%E5%91%BD%E4%BB%A4.html">25. Vim查找和替换命令</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell%E8%84%9A%E6%9C%AC%E6%97%A5%E6%9C%9F%E9%80%92%E5%A2%9E%28%E8%B5%B7%E6%AD%A2%E6%97%A5%E6%9C%9F%E5%86%85%E9%80%92%E5%A2%9E%29.html">26. Shell脚本日期递增(起止日期内递增)</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%85%B3%E4%BA%8EKudu%20Upsert%E5%88%97%E7%9A%84%E9%97%AE%E9%A2%98.html">27. 关于Kudu Upsert列的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%85%B3%E4%BA%8EKudu%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F%E7%9A%84%E4%BF%AE%E6%94%B9.html">28. 关于Kudu列的顺序的修改</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%AF%E8%83%BD%E6%9C%89%E7%94%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E9%93%BE%E6%8E%A5.html">29. 可能有用的学习链接</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%AF%E8%83%BD%E6%9C%89%E7%94%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E9%93%BE%E6%8E%A5.html#hive-orc">30. Hive - ORC 文件存储格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%A4%9A%E7%BA%BF%E7%A8%8B.html">31. 多线程</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%B0%86Spark%20DataFrame%E4%B8%AD%E7%9A%84%E6%95%B0%E5%80%BC%E5%8F%96%E5%87%BA.html">32. 将Spark DataFrame中的数值取出</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E6%89%93%E5%8D%B0%E6%9C%AC%E6%9C%BAIP.html">33. 打印本机IP</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5%E9%9C%80%E8%A6%81Kerberos%E8%AE%A4%E8%AF%81%E7%9A%84Hive.html">34. 本地连接需要Kerberos认证的Hive</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98.html">35. 生产者消费者模型问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html">36. JAVA设计模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html#continuing">37. Continuing…</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E5%8E%9F%E5%88%99.html">38. 设计模式六大原则</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E6%A2%B3%E7%90%86.html">39. 面向对象知识点梳理</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%90%84%E7%A7%8D%E5%85%B3%E7%B3%BB.html">40. Java OOP防脱发指南</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Blogs</a> &raquo;</li>
        
      <li><span class="section-number">1. </span>面试题整理</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/Blog Here/0904模拟面试题整理.md.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1><span class="section-number">1. </span>面试题整理<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<div class="section" id="hadoopsplitblock">
<h2><span class="section-number">1.1. </span>1.Hadoop中Split和Block的区别？(<a class="reference external" href="https://www.jianshu.com/p/3e3758519777">看这篇博客</a>)<a class="headerlink" href="#hadoopsplitblock" title="永久链接至标题">¶</a></h2>
<ul>
<li><p>Block</p>
<ul>
<li><p>文件上传到HDFS时会被分块(Block)，是真实的物理块的划分。默认块大小是128MB</p></li>
<li><p>分布式存储系统中选择大的Block size主要是为了<strong>最小化寻址开销</strong>，使得磁盘传输数据的时间可以明显大于定于这个块所需要的时间。</p>
<blockquote>
<div><p>在磁盘中，每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位，磁盘块一般为<strong>512字节</strong>。但是在分布式文件系统中数据块一般远大于磁盘数据块的大小，并且为磁盘块大小的整数倍，例如HDFS Block size默认为64MB。</p>
<p>在HDFS中Block size也不好设置的过大，这是因为MapReduce中的map任务通常一次处理一个块中的数据，因此如果Block太大，则map数就会减少，作业运行的并行度就会受到影响，速度就会较慢。</p>
<p>当上传到HDFS上的文件大小小于Block size时，会按文件实际大小存储。</p>
</div></blockquote>
</li>
</ul>
</li>
<li><p>Split</p>
<ul class="simple">
<li><p>Split是逻辑意义上的切分，文件并没有真正意义上在物理层面被切分。</p></li>
<li><p>Split size允许用户自定义，默认为HDFS的Block Size</p></li>
<li><p>在Input阶段，MapReduce会根据输入文件计算输入分片，每个split就对应一个Map</p></li>
</ul>
</li>
<li><p>总结：</p>
<ul class="simple">
<li><p>Block是物理上的分割，而Split是逻辑上的切割</p></li>
<li><p>一个split可以包含多个block，一个block也可应用多个split，这取决于用户自定义的split size</p></li>
<li><p>一个split对应于一个map</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="hive">
<h2><span class="section-number">1.2. </span>2.Hive中如何处理小文件？<a class="headerlink" href="#hive" title="永久链接至标题">¶</a></h2>
<ul>
<li><p>产生原因：</p>
<ul class="simple">
<li><p>数据源本来就包含大量的小文件</p></li>
<li><p>Reduce数量越多，小文件也越多</p></li>
<li><p>动态分区插入数据，产生大量的小文件，从而导致map数量剧增</p></li>
</ul>
</li>
<li><p>影响：</p>
<ul class="simple">
<li><p>1.从Hive的角度看，小文件会开很多map，一个map开一个JVM去执行，所以这些任务的初始化，启动，执行会浪费大量的资源，严重影响性能。</p></li>
<li><p>2.在HDFS中，每个小文件对象约占150byte，如果小文件过多会占用大量内存。这样NameNode内存容量严重制约了集群的扩展。</p></li>
</ul>
</li>
<li><p>解决：</p>
<ul>
<li><p>1.使用hadoop archive命令把小文件进行归档。</p></li>
<li><p>2.重建表，建表时减少reduce数量。</p></li>
<li><p>3.通过参数进行调节，设置map/reduce端的相关参数</p>
<ul>
<li><p>设置map输入合并小文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">//</span><span class="n">每个Map最大输入大小</span><span class="p">(</span><span class="n">这个值决定了合并后文件的数量</span><span class="p">)</span>
<span class="nb">set</span> <span class="n">mapred</span><span class="o">.</span><span class="n">max</span><span class="o">.</span><span class="n">split</span><span class="o">.</span><span class="n">size</span><span class="o">=</span><span class="mi">256000000</span><span class="p">;</span>
<span class="o">//</span><span class="n">一个节点上split的至少的大小</span><span class="p">(</span><span class="n">这个值决定了多个DataNode上的文件是否需要合并</span><span class="p">)</span>
<span class="nb">set</span> <span class="n">mapred</span><span class="o">.</span><span class="n">min</span><span class="o">.</span><span class="n">split</span><span class="o">.</span><span class="n">size</span><span class="o">.</span><span class="n">per</span><span class="o">.</span><span class="n">node</span><span class="o">=</span><span class="mi">100000000</span><span class="p">;</span>
<span class="o">//</span><span class="n">一个交换机下split的至少的大小</span><span class="p">(</span><span class="n">这个值决定了多个交换机上的文件是否需要合并</span><span class="p">)</span>
<span class="nb">set</span> <span class="n">mapred</span><span class="o">.</span><span class="n">min</span><span class="o">.</span><span class="n">split</span><span class="o">.</span><span class="n">size</span><span class="o">.</span><span class="n">per</span><span class="o">.</span><span class="n">rack</span><span class="o">=</span><span class="mi">100000000</span><span class="p">;</span>
<span class="o">//</span><span class="n">执行Map前进行小文件合并</span>
<span class="nb">set</span> <span class="n">hive</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">format</span><span class="o">=</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">hadoop</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="n">ql</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">CombineHiveInputFormat</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>设置map输出和reduce输出合并文件</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>//设置map端输出进行合并，默认为true
set hive.merge.mapfiles = true
//设置reduce端输出进行合并，默认为false
set hive.merge.mapredfiles = true
//设置合并文件的大小
set hive.merge.size.per.task = 256*1000*1000
//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。
set hive.merge.smallfiles.avgsize=16000000
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id2">
<h2><span class="section-number">1.3. </span>3.推测执行机制？<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>背景：</p>
<ul>
<li><p>MapReduce模型将作业分解成任务然后并行执行任务以使得整体运行时间少于各个任务顺序执行的时间</p></li>
<li><p>当一个作业(Job)由几百或者几千个任务组成时，可能出现少数“拖后腿”的任务，这些任务的执行时间过长</p>
<ul>
<li><p>原因有很多种，检测却很难，任务总能完成但是就是比预期的时间要长</p></li>
</ul>
</li>
</ul>
</li>
<li><p>解决：</p>
<ul>
<li><p>Hadoop不会去尝试或者诊断修复执行慢的任务，相反在一个任务运行比预期慢的时候，会尽量检测，并启动另一个相同的任务作为备份，这就是所谓的“<strong>推测执行</strong>”</p></li>
<li><p>如果同时执行两个重复的任务，会相互金正，导致推测执行无法执行，这对集群资源是一种浪费。调度器时刻跟踪作业中所有相同类型的任务的进度，并且仅仅启动运行速度明显低于平均水平的那一小部分的推测副本，一个任务完成以后，任何正在执行的重复任务都将被中止</p></li>
<li><p><strong>如果原任务在推测执行前完成，推测任务就被终止；如果推测任务先完成，原任务就被终止</strong></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="hadoop-java-java-object-serialization">
<h2><span class="section-number">1.4. </span>4.Hadoop为什么要自己实现序列化，而不用Java自带的序列化？（为什么不用Java Object Serialization？）<a class="headerlink" href="#hadoop-java-java-object-serialization" title="永久链接至标题">¶</a></h2>
<ul>
<li><p>Java有自己的序列化机制，成为Java Object Serialization，该机制与编程语言紧密相关。</p></li>
<li><p>Hadoop之父Doug Cutting是这样说的：“为什么开始设计Hadoop的时候不用Java Serialization？因为它**(Java Serialization)看起来太复杂，而我认为需要一个至精至简的机制，可以用于精确控制对象的读和写，这个机制将是Hadoop的核心**，使用Java Serialization虽然可以获得一些控制权，但用起来非常纠结。不用RMI(Remote Mwthod Invocation 远程方法调用)也处于类似的考虑。<strong>高效、高性能的进程间通信是Hadoop的关键</strong>。我觉得我们需要精确控制连接、延迟和缓冲的处理方式，RWI对此也无能为力。”</p></li>
<li><p>问题在于Java Serialization不满足Hadoop序列化格式标准：精简、快速、可扩展、支持互操作</p>
<blockquote>
<div><p>简而言之，Java自带的序列化太过于笨重，不能实现Hadoop中所要求的高效、高性能的进程间通信和精确控制对象的读和写。</p>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="yarn">
<h2><span class="section-number">1.5. </span>5.yarn的三种调度策略?<a class="headerlink" href="#yarn" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>FIFO Scheduler</p></li>
<li><p>Capacity Scheduler</p></li>
<li><p>FairScheduler</p></li>
</ul>
</div>
<div class="section" id="blocksize">
<h2><span class="section-number">1.6. </span>6.blocksize是根据什么设置?<a class="headerlink" href="#blocksize" title="永久链接至标题">¶</a></h2>
</div>
<div class="section" id="id3">
<h2><span class="section-number">1.7. </span>7.分布式缓存?<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
</div>
<div class="section" id="id4">
<h2><span class="section-number">1.8. </span>8.blocksize是根据什么设置?<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
</div>
<div class="section" id="hivelock">
<h2><span class="section-number">1.9. </span>9.hive支持lock吗?<a class="headerlink" href="#hivelock" title="永久链接至标题">¶</a></h2>
<p>1.hive锁表命令</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hive&gt;</span> <span class="pre">lock</span> <span class="pre">table</span> <span class="pre">t1</span> <span class="pre">exclusive;锁表后不能对表进行操作</span></code></p></li>
</ol>
<p>2.hive表解锁：</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hive&gt;</span> <span class="pre">unlock</span> <span class="pre">table</span> <span class="pre">t1;</span></code></p></li>
</ol>
<p>3.查看被锁的表</p>
<p>1.hive&gt; show locks;</p>
</div>
<div class="section" id="hdfsblock">
<h2><span class="section-number">1.10. </span>10.为什么HDFS的Block不能设置太大也不能设置太小？<a class="headerlink" href="#hdfsblock" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>如果块设置过大</p>
<ul>
<li><p>一方面，从磁盘传输数据的时间会明显大于寻址时间，导致程序在处理这块数据时，变得非常慢；</p></li>
<li><p>另一方面，mapreduce中的map任务通常一次只处理一个块中的数据，如果块过大运行速度也会很慢。</p></li>
</ul>
</li>
<li><p>如果块设置过小</p>
<ul>
<li><p>一方面存放大量小文件会占用NameNode中大量内存来存储元数据，而NameNode的内存是有限的，不可取；</p></li>
<li><p>另一方面文件块过小，寻址时间增大，导致程序一直在找block的开始位置。</p></li>
</ul>
</li>
<li><p>因而，块适当设置大一些，减少寻址时间，那么传输一个由多个块组成的文件的时间****主要取决于磁盘的传输速率****。</p></li>
</ul>
</div>
<div class="section" id="hdfs-block-128m">
<h2><span class="section-number">1.11. </span>11.HDFS中块（block）的大小为什么设置为128M？<a class="headerlink" href="#hdfs-block-128m" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>答案来自：<a class="reference external" href="https://blog.csdn.net/wx1528159409/article/details/84260023">点我看博客原文</a></p>
</div></blockquote>
<ol class="simple">
<li><p>HDFS中平均寻址时间大概为10ms；</p></li>
<li><p>经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态；</p></li>
</ol>
<p>所以最佳传输时间为10ms/0.01=1000ms=1s</p>
<ol class="simple">
<li><p>目前磁盘的传输速率普遍为100MB/s；</p></li>
</ol>
<p>计算出最佳block大小：100MB/s x 1s = 100MB</p>
<p>所以我们设定block大小为128MB。</p>
</div>
<div class="section" id="id5">
<h2><span class="section-number">1.12. </span>12.Hive严格模式和非严格模式的区别？<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Hive提供了一个严格模式，可以防止用户执行那些可能产生意想不到的不好的效果的查询。即某些查询在严格
模式下无法执行。通过设置hive.mapred.mode的值为strict，可以禁止3中类型的查询。</p>
</div></blockquote>
<ul class="simple">
<li><p>带有分区的表的查询</p>
<ul>
<li><p>如果在一个分区表执行hive，除非where语句中包含分区字段过滤条件来显示数据范围，否则不允许执行。换句话说，就是<strong>不允许用户扫描所有的分区</strong>。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。如果没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表</p></li>
</ul>
</li>
<li><p>带有orderby的查询</p>
<ul>
<li><p>对于使用了orderby的查询，要求必须有limit语句。因为<strong>order by为了执行排序过程会讲所有的结果分发到同一个reducer中进行处理</strong>，强烈要求用户增加这个limit语句可以防止reducer额外执行很长一段时间</p></li>
</ul>
</li>
<li><p>限制笛卡尔积的查询</p>
<ul>
<li><p>对关系型数据库非常了解的用户可能期望在执行join查询的时候不使用on语句而是使用where语句，这样关系数据库的执行优化器就可以高效的将where语句转换成那个on语句。不幸的是，hive不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况</p></li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li><p><strong>1.对分区表进行查询，必须使用where+分区字段来限制范围。</strong></p></li>
<li><p><strong>2.使用orderby查询的时候，必须加上limit限制，因为执行order by的时候，已经将所有的数据放到了一个reduce中了。</strong></p></li>
<li><p><strong>3.限制笛卡尔积的查询，因为在关系型数据库中，可以使用where充当on，但是在hive数据仓库中，必须使用on。</strong></p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>非严格模式则没有上述限制</p>
</div></blockquote>
</div>
<div class="section" id="jvm">
<h2><span class="section-number">1.13. </span>13.JVM的重用机制<a class="headerlink" href="#jvm" title="永久链接至标题">¶</a></h2>
<ul>
<li><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免<strong>小文件</strong>的场景或<strong>task特别多的场景</strong>，这类场景大多数执行时间都很短。</p></li>
<li><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。<strong>JVM重用可以使得JVM实例在同一个job中重新使用N次</strong>。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>mapreduce.job.jvm.numtasks<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;description&gt;</span>How many tasks to run per jvm. If set to -1, there is no limit. <span class="nt">&lt;/description&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</pre></div>
</div>
</li>
<li><p>缺点：</p>
<ul class="simple">
<li><p>开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id6">
<h2><span class="section-number">1.14. </span>14.Hive文件存储格式及他们的类型(基于行还是列)？<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>博文参考 <a class="reference external" href="https://www.cnblogs.com/sunpengblog/p/11912958.html">hive中parquet和SEQUENCEFILE区别</a></p>
</div></blockquote>
<ul class="simple">
<li><p>Hive中的存储格式</p>
<ul>
<li><p>TEXTFILE</p>
<ul>
<li><p>默认格式</p></li>
</ul>
</li>
<li><p>SQUENCEFILE</p></li>
<li><p>RCFILE</p></li>
<li><p>ORCFILE</p></li>
<li><p>PARQUET</p></li>
</ul>
</li>
<li><p>分类</p>
<ul>
<li><p>基于行存储</p>
<ul>
<li><p>TEXTFILE</p></li>
<li><p>SQUENCEFILE</p>
<ul>
<li><p>存储为二进制</p></li>
</ul>
</li>
<li><p>RCFILE</p>
<ul>
<li><p>行列混合</p></li>
</ul>
</li>
</ul>
</li>
<li><p>基于列式存储</p>
<ul>
<li><p>ORC</p></li>
<li><p>PARQUET</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id7">
<h2><span class="section-number">1.15. </span>15.分区与分桶的区别？<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>分区针对的是数据，分桶针对的是文件，不同分区就是不同的目录。</p></li>
<li><p>分桶针对的是文件，一个桶对应的就是一个文件</p>
<ul>
<li><p>分桶规则，按照某列的值的Hash取余桶的个数</p></li>
<li><p>本质：低层就是MapReduce的分区（多个reduce的情况下）</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id8">
<h2><span class="section-number">1.16. </span>16.Yarn的产生是为了解决什么问题？<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h2>
<p>在Hadoop 1.x中负责资源调度的是JobTracker，当存在多个计算框架时，每个框架都有自己的资源调度工具，这就会引起资源争抢，造成故障，在Hadoop 2.x中引入了Yarn，负责<strong>提供统一的资源服务</strong>。</p>
</div>
<div class="section" id="hiveorder-bysort-bydistribute-bycluster-by">
<h2><span class="section-number">1.17. </span>17.Hive中order by、sort by、distribute by和cluster by的区别和联系？<a class="headerlink" href="#hiveorder-bysort-bydistribute-bycluster-by" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>order by</p>
<ul>
<li><p>全局排序</p></li>
<li><p>在严格模式下使用order by必须使用LIMIT</p></li>
</ul>
</li>
<li><p>sort by</p>
<ul>
<li><p>单独在各自的reduce中排序，不能保证全局有序</p></li>
<li><p>一般和distribute by一起使用，并且书写在distribute by之后</p></li>
<li><p>当reduce个数为1(mapred.reduce.tasks=1)效果个order by一样，当reduce为多个时，，每个reduce输出一个文件，文件内部有序，但不能全局有序</p></li>
</ul>
</li>
<li><p>distribute by</p>
<ul>
<li><p>控制Map输出的结果被怎样划分进入Reducer</p></li>
<li><p>相同Key的值进入同一个Reducer</p></li>
</ul>
</li>
<li><p>cluster by</p>
<ul>
<li><p>相当远distribute by  … sort by…相结合使用</p></li>
<li><p>只能升序排序</p></li>
</ul>
</li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="1008%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86.html" class="btn btn-neutral float-right" title="2. 实时存储NoSQL面试" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="Blogs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2020-2021, roohom.

    </p>
  </div>
    
    
    
    利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    
    由 <a href="https://readthedocs.org">Read the Docs</a>开发. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>