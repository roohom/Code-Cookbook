

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>19. PySpark &mdash; Code-Cookbook 0.1 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="20. PySpark On Yarn" href="PySparkOnYarn.html" />
    <link rel="prev" title="18. 使用Java在服务端和客户端之间传送文件" href="Java%E4%B8%ADSocket%E5%8F%91%E9%80%81%E6%96%87%E4%BB%B6%E8%87%B3%E6%9C%8D%E5%8A%A1%E7%AB%AF.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Code-Cookbook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">大数据</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bigdata%20Tools/index.html">Bigdata Tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">博客</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Blogs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="0904%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E9%A2%98%E6%95%B4%E7%90%86.html">1. 面试题整理</a></li>
<li class="toctree-l2"><a class="reference internal" href="1008%E6%A8%A1%E6%8B%9F%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86.html">2. 实时存储NoSQL面试</a></li>
<li class="toctree-l2"><a class="reference internal" href="Annotation.html">3. 元注解</a></li>
<li class="toctree-l2"><a class="reference internal" href="Annotation.html#id2">4. 注解解析</a></li>
<li class="toctree-l2"><a class="reference internal" href="Closure%26Currying.html">5. Scala函数中闭包(Closure)和柯里化(Currying)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Collection.html">6. 集合</a></li>
<li class="toctree-l2"><a class="reference internal" href="Collection.html#id9">7. 简单(常用)数据结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="Commands.html">8. Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="EOF.html">9. EOF</a></li>
<li class="toctree-l2"><a class="reference internal" href="Git.html">10. Git问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hadoop%20distcp.html">11. Hadoop distcp</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hive%20SQL50%E9%A2%98%E8%AE%B0%E5%BD%95.html">12. Hive SQL50题记录</a></li>
<li class="toctree-l2"><a class="reference internal" href="IO%E6%B5%81.html">13. <code class="docutils literal notranslate"><span class="pre">IO</span> <span class="pre">Stream</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="Java%20Cookbook.html">14. JAVA Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java8%20Stream%28%29.html">15. Java8 Stream API</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java_Maven.html">16. Maven</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java_OOP.html">17. Java_OOP</a></li>
<li class="toctree-l2"><a class="reference internal" href="Java%E4%B8%ADSocket%E5%8F%91%E9%80%81%E6%96%87%E4%BB%B6%E8%87%B3%E6%9C%8D%E5%8A%A1%E7%AB%AF.html">18. 使用Java在服务端和客户端之间传送文件</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">19. PySpark</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">19.1. 先决条件</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#step1">19.1.1. Step1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step2">19.1.2. Step2</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step3">19.1.3. Step3</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id2">19.2. 即刻开始</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">19.2.1. 运气好</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">19.2.2. 运气不好</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">19.3. 正式使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pysparkkudu">19.4. PySpark读取Kudu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pysparkhive">19.5. PySpark读取Hive</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="PySparkOnYarn.html">20. PySpark On Yarn</a></li>
<li class="toctree-l2"><a class="reference internal" href="SimpleDataStruct.html">21. 简单(常用)数据结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="Socket.html">22. Socket</a></li>
<li class="toctree-l2"><a class="reference internal" href="Spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1RSA%20premaster%20secret%20error.html">23. Spark提交任务RSA premaster secret error</a></li>
<li class="toctree-l2"><a class="reference internal" href="Springboot%E6%95%B4%E5%90%88Spark%2C%20%E6%9C%AC%E5%9C%B0%E3%80%81%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html">24. Springboot整合Spark, 本地、集群部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="Vim%E6%9F%A5%E6%89%BE%E5%92%8C%E6%9B%BF%E6%8D%A2%E5%91%BD%E4%BB%A4.html">25. Vim查找和替换命令</a></li>
<li class="toctree-l2"><a class="reference internal" href="shell%E8%84%9A%E6%9C%AC%E6%97%A5%E6%9C%9F%E9%80%92%E5%A2%9E%28%E8%B5%B7%E6%AD%A2%E6%97%A5%E6%9C%9F%E5%86%85%E9%80%92%E5%A2%9E%29.html">26. Shell脚本日期递增(起止日期内递增)</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%85%B3%E4%BA%8EKudu%20Upsert%E5%88%97%E7%9A%84%E9%97%AE%E9%A2%98.html">27. 关于Kudu Upsert列的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%85%B3%E4%BA%8EKudu%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F%E7%9A%84%E4%BF%AE%E6%94%B9.html">28. 关于Kudu列的顺序的修改</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%AF%E8%83%BD%E6%9C%89%E7%94%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E9%93%BE%E6%8E%A5.html">29. 可能有用的学习链接</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%8F%AF%E8%83%BD%E6%9C%89%E7%94%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E9%93%BE%E6%8E%A5.html#hive-orc">30. Hive - ORC 文件存储格式</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%A4%9A%E7%BA%BF%E7%A8%8B.html">31. 多线程</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%B0%86Spark%20DataFrame%E4%B8%AD%E7%9A%84%E6%95%B0%E5%80%BC%E5%8F%96%E5%87%BA.html">32. 将Spark DataFrame中的数值取出</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E6%89%93%E5%8D%B0%E6%9C%AC%E6%9C%BAIP.html">33. 打印本机IP</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5%E9%9C%80%E8%A6%81Kerberos%E8%AE%A4%E8%AF%81%E7%9A%84Hive.html">34. 本地连接需要Kerberos认证的Hive</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98.html">35. 生产者消费者模型问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html">36. JAVA设计模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html#continuing">37. Continuing…</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E5%8E%9F%E5%88%99.html">38. 设计模式六大原则</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E6%A2%B3%E7%90%86.html">39. 面向对象知识点梳理</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%90%84%E7%A7%8D%E5%85%B3%E7%B3%BB.html">40. Java OOP防脱发指南</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Blogs</a> &raquo;</li>
        
      <li><span class="section-number">19. </span>PySpark</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/Blog Here/PySpark.md.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pyspark">
<h1><span class="section-number">19. </span>PySpark<a class="headerlink" href="#pyspark" title="永久链接至标题">¶</a></h1>
<blockquote>
<div><p>REFERENCE: <a class="reference external" href="http://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html">官方文档</a></p>
</div></blockquote>
<div class="section" id="id1">
<h2><span class="section-number">19.1. </span>先决条件<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>以下所说的都为在Windows环境测试</p>
</div></blockquote>
<p>环境为<strong>Anaconda</strong>、<strong>Spark2.4.7</strong>、<strong>JDK1.8</strong>、<strong>Python3.7</strong>、<strong>PySpark2.4.7</strong></p>
<p>Anaconda是一个杰出的数据分析工具，内部集成了大量的有关数据分析的Python包，方便使用，我们使用Anaconda来安装PySpark，Anaconda默认自带Python环境，此处默认已经配置好了Python环境</p>
<div class="section" id="step1">
<h3><span class="section-number">19.1.1. </span>Step1<a class="headerlink" href="#step1" title="永久链接至标题">¶</a></h3>
<p>在本地安装JDK并配置环境变量</p>
</div>
<div class="section" id="step2">
<h3><span class="section-number">19.1.2. </span>Step2<a class="headerlink" href="#step2" title="永久链接至标题">¶</a></h3>
<p>下载合适版本的Spark，并将Spark放置在合适目录，并配置环境变量</p>
<p><a class="reference external" href="http://spark.apache.org/downloads.html">去下载</a></p>
<p><img alt="image-20210315222441272" src="../_images/image-20210315222441272.png" /></p>
<blockquote>
<div><p>NOTE：Spark2除了Spark2.4.2是用Scala2.12预编译的之外其他都是用Scala2.11，Spark3.0+都是用Scala2.12预编译的</p>
</div></blockquote>
</div>
<div class="section" id="step3">
<h3><span class="section-number">19.1.3. </span>Step3<a class="headerlink" href="#step3" title="永久链接至标题">¶</a></h3>
<p>在本地安装PySpark，<strong>注意PySpark版本需要与Spark版本保持一致</strong>，如果不一致可能会带来意想不到的错误</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip uninstall pyspark
pip install <span class="nv">pyspark</span><span class="o">==</span><span class="m">2</span>.4.7
</pre></div>
</div>
<p>安装与Spark版本一致的PySpark，否则运行时会报错，可能会报Scala版本不一致的错误</p>
</div>
</div>
<div class="section" id="id2">
<h2><span class="section-number">19.2. </span>即刻开始<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<div class="section" id="id3">
<h3><span class="section-number">19.2.1. </span>运气好<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>如果运气好的话，打开Anaconda，从中打开Jupyter Lab或者Jupyter Notebook，使用以下代码即刻快速开始使用PySpark进行数据分析</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">date</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string1&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string2&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string3&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="p">])</span>
<span class="n">df</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DataFrame</span><span class="p">[</span><span class="n">a</span><span class="p">:</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">double</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">string</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="n">date</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">timestamp</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h3><span class="section-number">19.2.2. </span>运气不好<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>当然也有可能，由于安装存在种种问题，在使用上述代码进行开发的时候，总是报种种错误，下面这些代码是经过总结得到的，一般绝不会出错</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="c1"># 路径需要改为自己的</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
<span class="c1"># 路径改为自己本地的SPARK_HOME路径</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[1]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT 1 AS test&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">19.3. </span>正式使用<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>上述代码不适合复用，我们将其封装成为函数</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initSparkSession</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    实例化一个SparkSession对象</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYSPARK_SUBMIT_ARGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;--jars kudu-spark2_2.11-1.13.0.jar pyspark-shell&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;JAVA_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;C:\Program Files\Java\jdk1.8.0_201&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>

    <span class="kn">import</span> <span class="nn">findspark</span>
    <span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySparkLocal&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spark</span>
</pre></div>
</div>
</div>
<div class="section" id="pysparkkudu">
<h2><span class="section-number">19.4. </span>PySpark读取Kudu<a class="headerlink" href="#pysparkkudu" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>如果想要成功读取Kudu，还需要Kudu-Spark的Jar包，放在合适的位置，在代码中由<code class="docutils literal notranslate"><span class="pre">os.environ[&quot;PYSPARK_SUBMIT_ARGS&quot;]</span> <span class="pre">=</span> <span class="pre">'--jars</span> <span class="pre">kudu-spark2_2.11-1.13.0.jar</span> <span class="pre">pyspark-shell'</span></code>指定，示例中使用的是<code class="docutils literal notranslate"><span class="pre">kudu-spark2_2.11-1.13.0.jar</span></code>,位置为代码所在的目录，当然也可以使用绝对路径</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initSparkSession</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    实例化一个SparkSession对象</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYSPARK_SUBMIT_ARGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;--jars kudu-spark2_2.11-1.13.0.jar pyspark-shell&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;JAVA_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;C:\Program Files\Java\jdk1.8.0_201&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>

    <span class="kn">import</span> <span class="nn">findspark</span>
    <span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySparkLocal&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spark</span>

<span class="k">def</span> <span class="nf">readKuduTable</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">kuduMaster</span><span class="p">,</span> <span class="n">kuduTable</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;org.apache.kudu.spark.kudu&quot;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kudu.table&quot;</span><span class="p">,</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">kuduTable</span><span class="p">))</span> \
            <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kudu.master&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">kuduMaster</span><span class="p">))</span> \
            <span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">df</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 实例化一个SparkSession对象</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">initSparkSession</span><span class="p">()</span>
    <span class="c1"># 将对象作为参数传入函数</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">readKuduTable</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;node1:7051&quot;</span><span class="p">,</span> <span class="s2">&quot;fox_tm_vehicle_series&quot;</span><span class="p">)</span>
    
    <span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1"># 将DF的内容打印</span>
    <span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;tab&quot;</span><span class="p">)</span> <span class="c1"># 创建临时表，方便后面使用sql</span>
	<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT id, name, name_alias FROM tab&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1"># 选取几个字段进行展示打印</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT COUNT(*) FROM tab&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span> <span class="c1"># 以pandas的表格形式展示</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM tab LIMIT 20&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span> <span class="c1"># 如果原表数据量太大，直接调用toPandas()的话，可能会导致Driver内存溢出，这里使用sql选取20行再以pandas的表格形式展示</span>
    
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="pysparkhive">
<h2><span class="section-number">19.5. </span>PySpark读取Hive<a class="headerlink" href="#pysparkhive" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>为了成功支持读取Hive，在Windows环境下，需要本地安装spark环境，并且在spark的conf目录下放入Hive配置中的hive-site.xml和hadoop集群的core-site.xml、core-site.xml配置文件</p></li>
</ul>
<blockquote>
<div><p>Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and core-site.xml (for HDFS configuration) file in conf/.</p>
</div></blockquote>
<p>代码示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initSparkHiveSession</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    实例化一个SparkSession对象</span>
<span class="sd">    为了成功支持读取Hive，在Windows环境下，需要本地安装spark环境，</span>
<span class="sd">    并且在spark的conf目录下放入Hive配置中的hive-site.xml和hadoop集群的core-site.xml、core-site.xml配置文件</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    
    <span class="c1"># 用来支持读Kudu的，在本程序所在同级目录下必须有kudu-spark2_2.11-1.13.0.jar这个jar包</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYSPARK_SUBMIT_ARGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;--jars kudu-spark2_2.11-1.13.0.jar pyspark-shell&#39;</span>
    <span class="c1"># 本地环境的JDK</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;JAVA_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;C:\Program Files\Java\jdk1.8.0_201&#39;</span>
    <span class="c1"># 本地环境的spark-hadoop集成包</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>
    
    <span class="kn">import</span> <span class="nn">findspark</span>
    <span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
    <span class="c1"># config中配置Hive在HDFS上的目录地址，当创建数据库或者向表中写入文件时会在此目录下进行操作 </span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span> \
            <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;SparkSQLHive&quot;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.warehouse.dir&quot;</span><span class="p">,</span> <span class="s2">&quot;/user/hive/warehouse&quot;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span> \
            <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spark</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 实例化一个SparkSession对象</span>
    <span class="c1"># spark = initSparkSession()</span>
    
    <span class="c1"># 将对象作为参数传入函数</span>
    <span class="c1"># df = readKuduTable(spark, &quot;node1:7051,node2:7051,node3:7051&quot;, &quot;fox_tc_error_code_mapping&quot;)</span>
    
    <span class="n">spark</span> <span class="o">=</span> <span class="n">initSparkHiveSession</span><span class="p">()</span>
    
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;USE DEFAULT&quot;</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SHOW TABLES&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="image-20210428231837889" src="../_images/image-20210428231837889.png" /></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="PySparkOnYarn.html" class="btn btn-neutral float-right" title="20. PySpark On Yarn" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="Java%E4%B8%ADSocket%E5%8F%91%E9%80%81%E6%96%87%E4%BB%B6%E8%87%B3%E6%9C%8D%E5%8A%A1%E7%AB%AF.html" class="btn btn-neutral float-left" title="18. 使用Java在服务端和客户端之间传送文件" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2020-2021, roohom.

    </p>
  </div>
    
    
    
    利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    
    由 <a href="https://readthedocs.org">Read the Docs</a>开发. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>