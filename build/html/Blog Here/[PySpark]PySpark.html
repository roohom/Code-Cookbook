<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>42. [Pyspark]PySpark &mdash; Code-Cookbook 0.2 文档</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="43. [PySpark]PySpark On Yarn" href="%5BPySpark%5DPySparkOnYarn.html" />
    <link rel="prev" title="41. [MongoDB]MongoDB基本查询" href="%5BMongoDB%5DMongoDB%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Code-Cookbook
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">大数据</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bigdata%20Tools/index.html">Bigdata Tools</a></li>
</ul>
<p class="caption"><span class="caption-text">博客</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Blogs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Boxed%20Error.html">1. [Springboot x spark]java.util.concurrent.ExecutionException: Boxed Error</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BApollo%5DApollo%20Config%20Center.html">2. [Apollo]Apollo Config Center</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BConfluent%5DConfluent.html">3. [Confluent]Confluent快速上手</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5DCommdLine%2BSpringboot%2Bflink%E6%97%A0%E6%B3%95%E6%8C%87%E5%AE%9A%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%90%AF%E5%8A%A8.html">4. [Flink]CommdLine+Springboot+flink无法指定配置文件启动</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5DFlink-connector-http.html">5. [Flink]Flink-connector-http</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5DFlinkSource.html">6. [Flink]Flink Sources</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5DProcessFunction%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%EF%BC%8C%E6%8A%9B%E5%87%BAInvalidProgramException.html">7. [Flink]ProcessFunction无法使用，抛出InvalidProgramException</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5D%E4%BD%BF%E7%94%A8%E7%8A%B6%E6%80%81%E7%AE%97%E5%AD%90%E5%B0%86stream%E8%81%9A%E5%90%88%E8%BE%93%E5%87%BA.html">8. [Flink]使用状态算子将stream聚合输出</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5D%E5%A6%82%E4%BD%95%E6%9B%B4%E9%80%9A%E7%94%A8%E5%9C%B0%E5%B0%86Kafka%28%E6%88%96%E5%85%B6%E4%BB%96%29%E6%95%B0%E6%8D%AE%E8%90%BD%E5%9C%B0Hive%EF%BC%9F.html">9. [Flink]如何更通用地将Kafka(或其他)数据落地Hive？</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BFlink%5D%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%8F%E5%88%97%E5%8C%96%E6%B6%88%E8%B4%B9Kafka%E6%95%B0%E6%8D%AE.html">10. [Flink]自定义序列化消费Kafka数据</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BGit%5DGit.html">11. [Git]Git问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BGit%5D%E8%AF%AF%E5%9C%A8Master%E5%88%86%E6%94%AF%E5%BC%80%E5%8F%91%E5%B9%B6commit%E6%97%A0%E6%B3%95push.html">12. [Git]误在Master分支开发并commit无法push</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BHadoop%5DHadoop%20distcp.html">13. [Hadoop]Hadoop distcp</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BHadoop%5D%E4%B8%80%E4%BA%9BHadoop%E9%97%AE%E9%A2%98.html">14. [Hadoop]一些Hadoop问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BHive%5DHive%E5%88%86%E5%8C%BA%E8%A1%A8%E6%89%B9%E9%87%8F%E5%88%A0%E9%99%A4%E5%88%86%E5%8C%BA.html">15. [Hive]Hive分区表批量删除分区</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BHive%5D%E5%9C%A8%E6%8C%87%E5%AE%9A%E4%BD%8D%E7%BD%AE%E6%B7%BB%E5%8A%A0%E5%AD%97%E6%AE%B5.html">16. [Hive]在指定位置添加字段</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BHive%5D%E5%A4%96%E9%83%A8%E8%A1%A8%E4%BF%AE%E6%94%B9%E4%B8%BA%E5%86%85%E9%83%A8%E8%A1%A8.html">17. [Hive]外部表修改为内部表</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BHive%5D%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5%E9%9C%80%E8%A6%81Kerberos%E8%AE%A4%E8%AF%81%E7%9A%84Hive.html">18. [Hive]本地连接需要Kerberos认证的Hive</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJAVA%5DJava%20Cookbook.html">19. [JAVA]JAVA Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DAnnotation.html">20. [Java]元注解</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DAnnotation.html#id1">21. 注解解析</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DCollection.html">22. [Java]集合</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DCollection.html#id8">23. 简单(常用)数据结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DIO%E6%B5%81.html">24. [Java]<code class="docutils literal notranslate"><span class="pre">IO</span> <span class="pre">Stream</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DJava8%20Stream.html">25. [Java]Java8 Stream API</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DOOP.html">26. [Java]Java_OOP</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DSocket.html">27. [Java]Socket</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5DSocket%E5%8F%91%E9%80%81%E6%96%87%E4%BB%B6%E8%87%B3%E6%9C%8D%E5%8A%A1%E7%AB%AF.html">28. [Java]使用Java在服务端和客户端之间传送文件</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E4%B8%89%E7%A7%8D%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%E5%BA%94%E7%94%A8%E4%BA%8E%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%90%AF%E5%8A%A8.html">29. [Java]三种策略模式应用于服务的启动</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E5%A4%9A%E7%BA%BF%E7%A8%8B.html">30. [Java]多线程</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%E9%97%AE%E9%A2%98.html">31. [Java]生产者消费者模型问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E8%AE%A9%E9%A1%B9%E7%9B%AE%E9%A1%BA%E5%88%A9%E8%AF%BB%E5%8F%96resources%E7%9B%AE%E5%BD%95%E4%B8%8B%E7%9A%84%E6%96%87%E4%BB%B6.html">32. [Java]让项目顺利读取resources目录下的文件</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html">33. [Java]设计模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html#continuing">34. Continuing…</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%85%AD%E5%A4%A7%E5%8E%9F%E5%88%99.html">35. [Java]设计模式六大原则</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E6%A2%B3%E7%90%86.html">36. [Java]面向对象知识点梳理</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BJava%5D%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%90%84%E7%A7%8D%E5%85%B3%E7%B3%BB.html">37. [Java]OOP防脱发指南</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BKerberos%5DMessage%20stream%20modified%20%2841%29%E9%94%99%E8%AF%AF.html">38. [Kerberos]Message stream modified (41)错误</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BKudu%5D%E5%85%B3%E4%BA%8EKudu%20Upsert%E5%88%97%E7%9A%84%E9%97%AE%E9%A2%98.html">39. [Kudu]关于Kudu Upsert列的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BKudu%5D%E5%85%B3%E4%BA%8EKudu%E5%88%97%E7%9A%84%E9%A1%BA%E5%BA%8F%E7%9A%84%E4%BF%AE%E6%94%B9.html">40. [Kudu]关于Kudu列的顺序的修改</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BMongoDB%5DMongoDB%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2.html">41. [MongoDB]MongoDB基本查询</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">42. [Pyspark]PySpark</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">42.1. 先决条件</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#step1">42.1.1. Step1</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step2">42.1.2. Step2</a></li>
<li class="toctree-l4"><a class="reference internal" href="#step3">42.1.3. Step3</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id2">42.2. 即刻开始</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">42.2.1. 运气好</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">42.2.2. 运气不好</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">42.3. 正式使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pysparkkudu">42.4. PySpark读取Kudu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pysparkhive">42.5. PySpark读取Hive</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="%5BPySpark%5DPySparkOnYarn.html">43. [PySpark]PySpark On Yarn</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSQL%5DSQLIn%26NotIn.html">44. [SQL]<code class="docutils literal notranslate"><span class="pre">IN</span></code> OR <code class="docutils literal notranslate"><span class="pre">NOT</span> <span class="pre">IN</span></code> , IS A PROBLEM</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSQL%5D%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E7%9A%84%60create_time%60%E5%92%8C%60update_time%60.html">45. [SQL]业务数据库中的<code class="docutils literal notranslate"><span class="pre">create_time</span></code>和<code class="docutils literal notranslate"><span class="pre">update_time</span></code>分析时的问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSQL%5D%E4%B8%BA%E4%BB%80%E4%B9%88LEFT%20JOIN%E5%90%8E%E6%80%BB%E6%95%B0%E5%8D%B4%E4%B8%8E%E5%8F%B3%E8%A1%A8%E7%9A%84%E6%80%BB%E6%95%B0%E4%B8%80%E6%A0%B7%E4%BA%86%EF%BC%9F.html">46. [SQL]为什么LEFT JOIN后总数却与右表的总数一样了？</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSQL%5D%E6%B1%82%E7%94%A8%E6%88%B7%E4%BB%BB%E6%84%8F%E5%A4%A9%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%28%E6%AF%8F%E5%A4%A9%E4%B8%BA%E7%AC%AC%E5%A4%9A%E5%B0%91%E5%A4%A9%E8%BF%9E%E7%BB%AD%E7%99%BB%E5%BD%95%29.html">47. [SQL]求用户任意天连续登录(每天为第多少天连续登录)</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSQL%5D%E8%AE%A1%E7%AE%97%E6%8C%87%E5%AE%9A%E6%97%A5%E6%9C%9F%E7%9A%84%E5%B9%B4-%E5%91%A8%28%E4%B8%BA%E6%9F%90%E5%B9%B4%E7%9A%84%E7%AC%AC%E5%A4%9A%E5%B0%91%E5%91%A8%29.html">48. [SQL]计算指定日期的<strong>年-周</strong>(为某年的第多少周)</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BScala%5DClosure%26Currying.html">49. [Scala]函数中闭包(Closure)和柯里化(Currying)</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BShell%5DEOF.html">50. [Shell]EOF</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BShell%5Dshell%E8%84%9A%E6%9C%AC%E6%97%A5%E6%9C%9F%E9%80%92%E5%A2%9E%28%E8%B5%B7%E6%AD%A2%E6%97%A5%E6%9C%9F%E5%86%85%E9%80%92%E5%A2%9E%29.html">51. [Shell]Shell脚本日期递增(起止日期内递增)</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BShell%5D%E6%89%93%E5%8D%B0%E6%9C%AC%E6%9C%BAIP.html">52. [Shell]打印本机IP</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSparkStreaming%5D%E6%B6%88%E8%B4%B9kafka%E5%86%99%E5%85%A5Hive%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98.html">53. [SparkStreaming]消费kafka写入Hive失败的问题Lease timeout of 0 seconds expired</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpark%5DSparkSQL%20%E5%88%97%E8%BD%AC%E8%A1%8C%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B9%E6%B3%95.html">54. [Spark]SparkSQL 列转行的一种方法</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpark%5DSparkSQLJDBC%E5%B9%B6%E5%8F%91%E8%BF%9E%E6%8E%A5%E8%AF%BB%E5%8F%96.html">55. [Spark]SparkSQL JDBC并发连接读取</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpark%5DSpark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1RSA%20premaster%20secret%20error.html">56. [Spark]Spark提交任务RSA premaster secret error</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpark%5DSpringboot%E6%95%B4%E5%90%88Spark%2C%20%E6%9C%AC%E5%9C%B0%E3%80%81%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2.html">57. [Spark]Springboot整合Spark, 本地、集群部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpark%5D%E4%BD%BF%E7%94%A8Java%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AARow.html">58. [Spark]如何使用Java创建一个Row</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpark%5D%E5%B0%86Spark%20DataFrame%E4%B8%AD%E7%9A%84%E6%95%B0%E5%80%BC%E5%8F%96%E5%87%BA.html">59. [Spark]将Spark DataFrame中的数值取出</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BSpringboot%5DokHttp%E9%94%99%E8%AF%AFException%20in%20thread%20OkHttp%20Dispatcher%20java.lang.IllegalStateException%20closed.html">60. [Springboot]okHttp错误:<em>Exception</em> in thread “OkHttp Dispatcher” <em>java.lang.IllegalStateException</em>: closed</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5BVim%5D%E6%9F%A5%E6%89%BE%E5%92%8C%E6%9B%BF%E6%8D%A2%E5%91%BD%E4%BB%A4.html">61. [Vim]Vim查找和替换命令</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5Bdebezium%5D%E5%9C%A8%E5%90%AF%E5%8A%A8%E4%BB%BB%E5%8A%A1%E6%97%B6%E4%BC%A0%E5%85%A5SQL%E8%AF%AD%E5%8F%A5%E7%94%9F%E6%88%90Snapshot.html">62. [debezium]在启动任务时传入SQL语句生成Snapshot</a></li>
<li class="toctree-l2"><a class="reference internal" href="%5Bdebezium%5D%E7%83%AD%E4%BF%AE%E6%94%B9DebeziumMySQLConnector%E9%85%8D%E7%BD%AE.html">63. [debezium]热修改Debezium MySQL Connector配置</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Blogs</a> &raquo;</li>
      <li><span class="section-number">42. </span>[Pyspark]PySpark</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/Blog Here/[PySpark]PySpark.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pyspark-pyspark">
<h1><span class="section-number">42. </span>[Pyspark]PySpark<a class="headerlink" href="#pyspark-pyspark" title="永久链接至标题"></a></h1>
<blockquote>
<div><p>REFERENCE: <a class="reference external" href="http://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html">官方文档</a></p>
</div></blockquote>
<section id="id1">
<h2><span class="section-number">42.1. </span>先决条件<a class="headerlink" href="#id1" title="永久链接至标题"></a></h2>
<blockquote>
<div><p>以下所说的都为在Windows环境测试</p>
</div></blockquote>
<p>环境为<strong>Anaconda</strong>、<strong>Spark2.4.7</strong>、<strong>JDK1.8</strong>、<strong>Python3.7</strong>、<strong>PySpark2.4.7</strong></p>
<p>Anaconda是一个杰出的数据分析工具，内部集成了大量的有关数据分析的Python包，方便使用，我们使用Anaconda来安装PySpark，Anaconda默认自带Python环境，此处默认已经配置好了Python环境</p>
<section id="step1">
<h3><span class="section-number">42.1.1. </span>Step1<a class="headerlink" href="#step1" title="永久链接至标题"></a></h3>
<p>在本地安装JDK并配置环境变量</p>
</section>
<section id="step2">
<h3><span class="section-number">42.1.2. </span>Step2<a class="headerlink" href="#step2" title="永久链接至标题"></a></h3>
<p>下载合适版本的Spark，并将Spark放置在合适目录，并配置环境变量</p>
<p><a class="reference external" href="http://spark.apache.org/downloads.html">去下载</a></p>
<p><img alt="image-20210315222441272" src="../_images/image-20210315222441272.png" /></p>
<blockquote>
<div><p>NOTE：Spark2除了Spark2.4.2是用Scala2.12预编译的之外其他都是用Scala2.11，Spark3.0+都是用Scala2.12预编译的</p>
</div></blockquote>
</section>
<section id="step3">
<h3><span class="section-number">42.1.3. </span>Step3<a class="headerlink" href="#step3" title="永久链接至标题"></a></h3>
<p>在本地安装PySpark，<strong>注意PySpark版本需要与Spark版本保持一致</strong>，如果不一致可能会带来意想不到的错误</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip uninstall pyspark
pip install <span class="nv">pyspark</span><span class="o">==</span><span class="m">2</span>.4.7
</pre></div>
</div>
<p>安装与Spark版本一致的PySpark，否则运行时会报错，可能会报Scala版本不一致的错误</p>
</section>
</section>
<section id="id2">
<h2><span class="section-number">42.2. </span>即刻开始<a class="headerlink" href="#id2" title="永久链接至标题"></a></h2>
<section id="id3">
<h3><span class="section-number">42.2.1. </span>运气好<a class="headerlink" href="#id3" title="永久链接至标题"></a></h3>
<p>如果运气好的话，打开Anaconda，从中打开Jupyter Lab或者Jupyter Notebook，使用以下代码即刻快速开始使用PySpark进行数据分析</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">date</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string1&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string2&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
    <span class="n">Row</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;string3&#39;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="n">date</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">e</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="p">])</span>
<span class="n">df</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DataFrame</span><span class="p">[</span><span class="n">a</span><span class="p">:</span> <span class="n">bigint</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">double</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">string</span><span class="p">,</span> <span class="n">d</span><span class="p">:</span> <span class="n">date</span><span class="p">,</span> <span class="n">e</span><span class="p">:</span> <span class="n">timestamp</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id4">
<h3><span class="section-number">42.2.2. </span>运气不好<a class="headerlink" href="#id4" title="永久链接至标题"></a></h3>
<p>当然也有可能，由于安装存在种种问题，在使用上述代码进行开发的时候，总是报种种错误，下面这些代码是经过总结得到的，一般绝不会出错</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">findspark</span>
<span class="c1"># 路径需要改为自己的</span>
<span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
<span class="c1"># 路径改为自己本地的SPARK_HOME路径</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[1]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT 1 AS test&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="id5">
<h2><span class="section-number">42.3. </span>正式使用<a class="headerlink" href="#id5" title="永久链接至标题"></a></h2>
<p>上述代码不适合复用，我们将其封装成为函数</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initSparkSession</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    实例化一个SparkSession对象</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYSPARK_SUBMIT_ARGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;--jars kudu-spark2_2.11-1.13.0.jar pyspark-shell&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;JAVA_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;C:\Program Files\Java\jdk1.8.0_201&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>

    <span class="kn">import</span> <span class="nn">findspark</span>
    <span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySparkLocal&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spark</span>
</pre></div>
</div>
</section>
<section id="pysparkkudu">
<h2><span class="section-number">42.4. </span>PySpark读取Kudu<a class="headerlink" href="#pysparkkudu" title="永久链接至标题"></a></h2>
<blockquote>
<div><p>如果想要成功读取Kudu，还需要Kudu-Spark的Jar包，放在合适的位置，在代码中由<code class="docutils literal notranslate"><span class="pre">os.environ[&quot;PYSPARK_SUBMIT_ARGS&quot;]</span> <span class="pre">=</span> <span class="pre">'--jars</span> <span class="pre">kudu-spark2_2.11-1.13.0.jar</span> <span class="pre">pyspark-shell'</span></code>指定，示例中使用的是<code class="docutils literal notranslate"><span class="pre">kudu-spark2_2.11-1.13.0.jar</span></code>,位置为代码所在的目录，当然也可以使用绝对路径</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initSparkSession</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    实例化一个SparkSession对象</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYSPARK_SUBMIT_ARGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;--jars kudu-spark2_2.11-1.13.0.jar pyspark-shell&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;JAVA_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;C:\Program Files\Java\jdk1.8.0_201&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>

    <span class="kn">import</span> <span class="nn">findspark</span>
    <span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[2]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PySparkLocal&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spark</span>

<span class="k">def</span> <span class="nf">readKuduTable</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">kuduMaster</span><span class="p">,</span> <span class="n">kuduTable</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;org.apache.kudu.spark.kudu&quot;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kudu.table&quot;</span><span class="p">,</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">kuduTable</span><span class="p">))</span> \
            <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;kudu.master&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">kuduMaster</span><span class="p">))</span> \
            <span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">df</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 实例化一个SparkSession对象</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">initSparkSession</span><span class="p">()</span>
    <span class="c1"># 将对象作为参数传入函数</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">readKuduTable</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s2">&quot;node1:7051&quot;</span><span class="p">,</span> <span class="s2">&quot;fox_tm_vehicle_series&quot;</span><span class="p">)</span>
    
    <span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1"># 将DF的内容打印</span>
    <span class="n">df</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;tab&quot;</span><span class="p">)</span> <span class="c1"># 创建临时表，方便后面使用sql</span>
	<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT id, name, name_alias FROM tab&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1"># 选取几个字段进行展示打印</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT COUNT(*) FROM tab&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span> <span class="c1"># 以pandas的表格形式展示</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM tab LIMIT 20&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span> <span class="c1"># 如果原表数据量太大，直接调用toPandas()的话，可能会导致Driver内存溢出，这里使用sql选取20行再以pandas的表格形式展示</span>
    
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="pysparkhive">
<h2><span class="section-number">42.5. </span>PySpark读取Hive<a class="headerlink" href="#pysparkhive" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p>为了成功支持读取Hive，在Windows环境下，需要本地安装spark环境，并且在spark的conf目录下放入Hive配置中的hive-site.xml和hadoop集群的core-site.xml、core-site.xml配置文件</p></li>
</ul>
<blockquote>
<div><p>Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and core-site.xml (for HDFS configuration) file in conf/.</p>
</div></blockquote>
<p>代码示例：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initSparkHiveSession</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    实例化一个SparkSession对象</span>
<span class="sd">    为了成功支持读取Hive，在Windows环境下，需要本地安装spark环境，</span>
<span class="sd">    并且在spark的conf目录下放入Hive配置中的hive-site.xml和hadoop集群的core-site.xml、core-site.xml配置文件</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    
    <span class="c1"># 用来支持读Kudu的，在本程序所在同级目录下必须有kudu-spark2_2.11-1.13.0.jar这个jar包</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYSPARK_SUBMIT_ARGS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;--jars kudu-spark2_2.11-1.13.0.jar pyspark-shell&#39;</span>
    <span class="c1"># 本地环境的JDK</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;JAVA_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;C:\Program Files\Java\jdk1.8.0_201&#39;</span>
    <span class="c1"># 本地环境的spark-hadoop集成包</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span> <span class="p">[</span><span class="s1">&#39;SPARK_HOME&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;E:\spark-2.4.7-bin-hadoop2.7&#39;</span>
    
    <span class="kn">import</span> <span class="nn">findspark</span>
    <span class="n">findspark</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="s2">&quot;E:\spark-2.4.7-bin-hadoop2.7&quot;</span><span class="p">)</span>
    <span class="c1"># config中配置Hive在HDFS上的目录地址，当创建数据库或者向表中写入文件时会在此目录下进行操作 </span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span> \
            <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;SparkSQLHive&quot;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.warehouse.dir&quot;</span><span class="p">,</span> <span class="s2">&quot;/user/hive/warehouse&quot;</span><span class="p">)</span> \
            <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span> \
            <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">spark</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="c1"># 实例化一个SparkSession对象</span>
    <span class="c1"># spark = initSparkSession()</span>
    
    <span class="c1"># 将对象作为参数传入函数</span>
    <span class="c1"># df = readKuduTable(spark, &quot;node1:7051,node2:7051,node3:7051&quot;, &quot;fox_tc_error_code_mapping&quot;)</span>
    
    <span class="n">spark</span> <span class="o">=</span> <span class="n">initSparkHiveSession</span><span class="p">()</span>
    
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;USE DEFAULT&quot;</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SHOW TABLES&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="image-20210428231837889" src="../_images/image-20210428231837889.png" /></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="%5BMongoDB%5DMongoDB%E5%9F%BA%E6%9C%AC%E6%9F%A5%E8%AF%A2.html" class="btn btn-neutral float-left" title="41. [MongoDB]MongoDB基本查询" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="%5BPySpark%5DPySparkOnYarn.html" class="btn btn-neutral float-right" title="43. [PySpark]PySpark On Yarn" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2020-2022, roohom.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>