

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>14. Spark Streaming &mdash; Code-Cookbook 0.1 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="15. Structured Streaming" href="StructuredStreaming.html" />
    <link rel="prev" title="13. Spark SQL" href="SparkSQL.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Code-Cookbook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">大数据</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Bigdata Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Apache%20Druid.html">1. Apache Druid</a></li>
<li class="toctree-l2"><a class="reference internal" href="Apache%20Flume.html">2. Apache Flume</a></li>
<li class="toctree-l2"><a class="reference internal" href="Flink.html">3. Flink</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hadoop%E6%90%AD%E5%BB%BA%E6%80%BB%E4%BD%93%E6%AD%A5%E9%AA%A4.html">4. Hadoop搭建总体步骤</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hbase.html">5. Hbase</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hive%E6%95%B0%E4%BB%93.html">6. Hive</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kafka.html">7. Kafka</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kudu.html">8. Kudu</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kylin.html">9. Kylin</a></li>
<li class="toctree-l2"><a class="reference internal" href="Redis.html">10. Redis</a></li>
<li class="toctree-l2"><a class="reference internal" href="Spark.html">11. Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="SparkCore.html">12. Spark Core</a></li>
<li class="toctree-l2"><a class="reference internal" href="SparkSQL.html">13. Spark SQL</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">14. Spark Streaming</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">14.1. 计算思想</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">14.2. 编程步骤</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">14.3. 应用监控</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">14.4. 工作原理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#streaming-context">14.4.1. Streaming Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="#receiver">14.4.2. Receiver接收数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="#blocks">14.4.3. 汇报接收Blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">14.4.4. 重要参数</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dstream">14.5. DStream</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id6">14.5.1. 函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">14.5.2. <strong>一个建议</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id8">14.6. 流式应用状态</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="StructuredStreaming.html">15. Structured Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Zookeeper.html">16. Zookeeper</a></li>
<li class="toctree-l2"><a class="reference internal" href="ZookeeperAndHadoop.html">17. ZookeeperAndHadoop</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BD%AF%E4%BB%B6%E5%90%AF%E5%8A%A8%E6%8C%87%E5%8D%97.html">18. 常用软件梳理</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">博客</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Blog%20Here/index.html">Blogs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Bigdata Tools</a> &raquo;</li>
        
      <li><span class="section-number">14. </span>Spark Streaming</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/Bigdata Tools/SparkStreaming.md.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark-streaming">
<h1><span class="section-number">14. </span>Spark Streaming<a class="headerlink" href="#spark-streaming" title="永久链接至标题">¶</a></h1>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><blockquote>
<div><p>Spark中针对流式数据处理的方案有：</p>
<ul class="simple">
<li><p>Spark Streaming</p></li>
<li><p>Structured Streaming（Spark SQL模块的部分功能：针对流式数据）</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><p><strong>Spark Streaming与Flink流式计算的本质区别是什么？</strong></p>
<p>Spark Streaming是基于微批(Micro batch)的，而Flink基于实时流，来一条数据处理一条数据</p>
</div></blockquote>
<div class="section" id="id1">
<h2><span class="section-number">14.1. </span>计算思想<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p><em>Spark Streaming makes it easy to build scalable fault-tolerent streaming applications.</em></p>
</div></blockquote>
<p>Spark Streaming是Spark生态系统当中一个重要的框架，它建立在Spark Core之上，<strong>Spark Streaming底层就是Spark Core 只不过是划分了微批</strong></p>
<p>对于Spark Streaming来说，将流式数据按照时间间隔BatchInterval划分为很多部分，每一部分Batch（批次），针对每批次数据Batch当做RDD进行快速分析和处理</p>
<p>它的核心是DStream[^1]，DStream类似于RDD，它实质上一系列的RDD的集合，DStream可以按照秒、分等时间间隔将数据流进行批量的划分。首先从接收到流数据之后，将其划分为多个batch，然后提交给Spark集群进行计算，最后将结果批量输出到HDFS或者数据库以及前端页面展示等等
$$
DStream = Seq[RDD]
$$</p>
<p>[^1]:  DStream(Discretized Stream，离散化数据流，连续不断的数据流)</p>
<p>将流式数据按照【X seconds】划分很多批次Batch，每个Batch数据封装到RDD中进行处理分析，最后每批次数据进行输出</p>
<blockquote>
<div><p>Notes：对于目前版本的Spark Streaming而言，其最小的Batch Size的选取在0.5~5秒钟之间，所以Spark Streaming能够满足流式准实时计算场景，对实时性要求非常高的如高频实时交易场景则不太适合。</p>
</div></blockquote>
</div>
<div class="section" id="id2">
<h2><span class="section-number">14.2. </span>编程步骤<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<ol>
<li><p>Define the input sources by creating input DStreams.
定义从哪个数据源接收流式数据，封装到DStream中</p></li>
<li><p>Define the streaming computations by applying transformation and output operations to DStreams.
针对业务调用DStream中函数，进行数据处理和输出[^2]</p></li>
<li><p>Start receiving data and processing it using streamingContext.start().</p>
<p>streamingContext对象的start()方法开始接收数据</p>
</li>
<li><p>Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().</p>
<p>使用streamingContext.awaitTermination()等待程序处理结束（手动或者是报出异常）</p>
</li>
<li><p>The processing can be manually stopped using streamingContext.stop().
启动流式应用，并且一直等待程序终止（人为或异常），最后停止运行</p></li>
</ol>
<p>[^2]:说了一大堆，就第二点最重要，第二点还不会写 :)</p>
</div>
<div class="section" id="id3">
<h2><span class="section-number">14.3. </span>应用监控<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>当流式处理程序运行时，可以通过主机名+端口号4040访问程序监控页面</p>
</div></blockquote>
<p><img alt="image-20201126161351293" src="../_images/image-20201126161351293.png" /></p>
<p>TD = SD + PD[^3]</p>
<p>$$
TD = SD + PD
$$</p>
<p>[^3]:每批次Batch数据处理总时间TD = 批次调度延迟时间SD + 批次数据处理时间PT</p>
<ul class="simple">
<li><p>性能衡量：</p>
<ul>
<li><p>每批次数据处理时间TD &lt;= BatchInterval每批次时间间隔</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id4">
<h2><span class="section-number">14.4. </span>工作原理<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>SparkStreaming处理流式数据时，按照时间间隔划分数据为微批次（Micro-Batch），每批次数据当做RDD，再进行处理分析。</p>
</div></blockquote>
<p><img alt="HowSparkStreamingWorks" src="../_images/HowSparkStreamingWorks.svg" /></p>
<div class="section" id="streaming-context">
<h3><span class="section-number">14.4.1. </span>Streaming Context<a class="headerlink" href="#streaming-context" title="永久链接至标题">¶</a></h3>
<p>当SparkStreaming流式应用启动（streamingContext.start）时，首先创建StreamingContext流式上下文实例对象，整个流式应用环境构建，底层还是SparkContext</p>
<p>当StreamingContext对象构建以后，启动接收器Receiver，专门从数据源端接收数据，<strong>此接收器作为Task任务运行在Executor中，一直运行（Long Runing），一直接收数据</strong></p>
<p>从WEB UI界面【Jobs Tab】可以看到【Job-0】是一个Receiver接收器，一直在运行，以Task方式运行，需要1Core CPU</p>
<p><img alt="image-20201126163124557" src="../_images/image-20201126163124557.png" /></p>
</div>
<div class="section" id="receiver">
<h3><span class="section-number">14.4.2. </span>Receiver接收数据<a class="headerlink" href="#receiver" title="永久链接至标题">¶</a></h3>
<p>启动每个接收器Receiver以后，实时从数据源端接收数据（比如TCP Socket），也是按照时间间隔将接收的流式数据划分为很多<strong>Block</strong>（块）</p>
<p>接收器 Receiver划分流式数据的时间间隔BlockInterval ，<strong>默认值为 200ms</strong>，通过属性【spark.streaming.blockInterval】设置。接收器将接收的数据划分为Block以后，按照设置的存储级别对Block进行存储，从TCP Socket中接收数据默认的存储级别为：MEMORY_AND_DISK_SER_2，先存储内存，不足再存储磁盘，存储2副本</p>
</div>
<div class="section" id="blocks">
<h3><span class="section-number">14.4.3. </span>汇报接收Blocks<a class="headerlink" href="#blocks" title="永久链接至标题">¶</a></h3>
<p>接收器Receiver将实时汇报接收的数据对应的Block信息，当BatchInterval时间达到以后，StreamingContext将对应时间范围内数据block当做RDD，加载SparkContext处理数据</p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">14.4.4. </span>重要参数<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>批次时间间隔BatchInterval[^4]</p>
<ul>
<li><p>每批次数据的时间间隔，每隔多久加载一个Job</p></li>
</ul>
</li>
<li><p>Block时间间隔：BlockInterval</p>
<ul>
<li><p>接收器划分流式数据的时间间隔，可以调整大小哦，官方建议最小值不能小于50ms</p></li>
<li><p>默认值为200ms，属性：spark.streaming.blockInterval，调整设置</p></li>
</ul>
</li>
</ul>
<p>[^4]:举个栗子：BatchInterval：1s = 1000ms = 5 * BlockInterval 每批次RDD数据中，有5个Block，每个Block就是RDD一个分区数据</p>
</div>
</div>
<div class="section" id="dstream">
<h2><span class="section-number">14.5. </span>DStream<a class="headerlink" href="#dstream" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>SparkStreaming模块将流式数据封装的数据结构：DStream（Discretized Stream，离散化数据流，连续不断的数据流），代表持续性的数据流和经过各种Spark算子操作后的结果数据流</p>
</div></blockquote>
<p><strong>那么DStream究竟是什么？</strong></p>
<blockquote>
<div><p>离散数据流（DStream）是Spark Streaming最基本的抽象。它代表了一种连续的数据流，要么从某种数据源提取数据，要么从其他数据流映射转换而来。DStream内部是由一系列连续的RDD组成的，每个RDD都包含了特定时间间隔内的一批数据</p>
</div></blockquote>
<p><strong>DStream<a class="reference external" href="DStream%E7%9B%B8%E5%BD%93%E4%BA%8E%E4%B8%80%E4%B8%AA%E5%BA%8F%E5%88%97%EF%BC%88%E9%9B%86%E5%90%88%EF%BC%89%EF%BC%8C%E9%87%8C%E9%9D%A2%E5%AD%98%E5%82%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%BARDD%EF%BC%88Streaming%E6%8C%89%E7%85%A7%E6%97%B6%E9%97%B4%E9%97%B4%E9%9A%94%E5%88%92%E5%88%86%E6%B5%81%E5%BC%8F%E6%95%B0%E6%8D%AE%EF%BC%89">^5</a>本质上是一个：一系列时间上连续的RDD（Seq[RDD]）</strong>
$$
DStream = Seq[RDD]
$$</p>
<p>DStream中每批次数据RDD在处理时，各个RDD之间存在依赖关系，DStream之间也有依赖关系，RDD具有容错性，那么DStream也具有容错性</p>
<div class="section" id="id6">
<h3><span class="section-number">14.5.1. </span>函数<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>主要有两大类：</p>
<p>一类是将一个DStream转换为另一个DStream的Transformation</p>
<p>另一类是DStream中每批次RDD经处理输出的Output Operations</p>
</div></blockquote>
<ul class="simple">
<li><p>Transformation函数</p>
<ul>
<li><p>map</p></li>
<li><p>flatmap</p></li>
<li><p>filter</p></li>
<li><p>count</p></li>
<li><p>reduce</p></li>
<li><p>countByKey</p></li>
<li><p>reduceByKey</p></li>
<li><p>union</p></li>
<li><p>join</p></li>
<li><p>cogroup</p></li>
</ul>
</li>
<li><p>Output函数</p>
<ul>
<li><p>print</p></li>
<li><p>saveAsTextFile</p></li>
<li><p>saveAsObjectFiles</p></li>
<li><p>saveAsHadoopFile</p></li>
<li><p>foreachRDD</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>注意：在DStream中有两个重要的函数，都是针对每批次数据RDD进行操作的，更加接近底层，性能
更好，强烈推荐使用</p>
<ul class="simple">
<li><p>转换函数transform：将一个DStream转换为另外一个DStream</p>
<ul>
<li><p><a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/rdd/StreamingTransformRDD.scala">代码模板</a></p></li>
</ul>
</li>
<li><p>输出函数foreachRDD：将一个DStream输出到外部存储系统</p>
<ul>
<li><p><a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/output/StreamingOutputRDD.scala">代码模板</a></p></li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id7">
<h3><span class="section-number">14.5.2. </span><strong>一个建议</strong><a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>对RDD操作的就不要对DStream操作，当调用DStream中某个函数在RDD中也存在，使用针对RDD操作</p>
</div>
</div>
<div class="section" id="id8">
<h2><span class="section-number">14.6. </span>流式应用状态<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h2>
<p>SparkStreaming流式计算框架，针对具体业务主要分为三类，使用不同函数进行处理：</p>
<ul class="simple">
<li><p>业务一：无状态Stateless</p>
<ul>
<li><p>使用<a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/rdd/StreamingTransformRDD.scala">transform</a>和<a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/output/StreamingOutputRDD.scala">foreachRDD</a>函数</p></li>
<li><p>比如实时增量数据ETL：实时从Kafka Topic中获取数据，经过初步转换操作，存储到Elasticsearch索引或HBase表中</p></li>
</ul>
</li>
<li><p>业务二：有状态State</p>
<ul>
<li><p>双十一大屏幕所有实时累加统计数字（比如销售额和销售量等），比如销售额、网站PV、UV等等</p></li>
<li><p>函数：<a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/app/state/StreamingUpdateState.scala">updateStateByKey</a>、<a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/app/state/StreamingMapWithState.scala">mapWithState</a></p></li>
</ul>
</li>
<li><p>业务三：窗口统计</p>
<ul>
<li><p>每隔多久时间统计最近一段时间内数据，比如饿了么后台报表，每隔5分钟统计最近20分钟订单数</p></li>
<li><p><a class="reference external" href="https://github.com/roohom/CodeIndex/blob/main/Spark/Spark-streaming/src/main/scala/me/iroohom/spark/app/window/StreamingWindow.scala">Window</a></p></li>
</ul>
</li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="StructuredStreaming.html" class="btn btn-neutral float-right" title="15. Structured Streaming" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="SparkSQL.html" class="btn btn-neutral float-left" title="13. Spark SQL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2020-2021, roohom.

    </p>
  </div>
    
    
    
    利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    
    由 <a href="https://readthedocs.org">Read the Docs</a>开发. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>