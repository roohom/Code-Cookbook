

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>11. Spark Core &mdash; Code-Cookbook 0.1 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="12. Spark SQL" href="SparkSQL.html" />
    <link rel="prev" title="10. Spark" href="Spark.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Code-Cookbook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">大数据</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Bigdata Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Apache%20Druid.html">1. Apache Druid</a></li>
<li class="toctree-l2"><a class="reference internal" href="Apache%20Flume.html">2. Apache Flume</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bigdata%E9%98%B2%E6%87%B5%E9%80%BC%E6%8C%87%E5%8D%97.html">3. Bigdata Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="Flink%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86.html">4. Flink基础配置与基础原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hadoop%E6%90%AD%E5%BB%BA%E6%80%BB%E4%BD%93%E6%AD%A5%E9%AA%A4.html">5. Hadoop搭建总体步骤</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hbase.html">6. Hbase</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kafka.html">7. Kafka</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kylin.html">8. Kylin</a></li>
<li class="toctree-l2"><a class="reference internal" href="Redis.html">9. Redis</a></li>
<li class="toctree-l2"><a class="reference internal" href="Spark.html">10. Spark</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11. Spark Core</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#rdd">11.1. RDD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">11.1.1. 特性</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">11.1.2. 函数</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id4">11.2. 持久化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">11.2.1. 缓存函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">11.2.2. 缓存级别</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">11.2.3. 释放缓存</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">11.2.4. 什么时候需要缓存数据？</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#checkpoint">11.3. Checkpoint</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">11.4. 共享变量</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">11.4.1. 广播变量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">11.4.2. 累加器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark">11.5. Spark内核调度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id12">11.5.1. RDD依赖</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stagedag">11.5.2. Stage和DAG</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark-shuffle">11.6. Spark Shuffle</a></li>
<li class="toctree-l3"><a class="reference internal" href="#job">11.7. Job调度流程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id16">11.8. 并行度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#task">11.8.1. 设置Task数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id17">11.8.2. 设置并行度</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="SparkSQL.html">12. Spark SQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="SparkStreaming.html">13. Spark Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="StructuredStreaming.html">14. Structured Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Zookeeper.html">15. Zookeeper</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BD%AF%E4%BB%B6%E5%90%AF%E5%8A%A8%E6%8C%87%E5%8D%97.html">16. 常用软件梳理</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">博客</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Blogs/index.html">Blogs</a></li>
</ul>
<p class="caption"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Bigdata Tools</a> &raquo;</li>
        
      <li><span class="section-number">11. </span>Spark Core</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/Bigdata Tools/SparkCore.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark-core">
<h1><span class="section-number">11. </span>Spark Core<a class="headerlink" href="#spark-core" title="永久链接至标题">¶</a></h1>
<div class="section" id="rdd">
<h2><span class="section-number">11.1. </span>RDD<a class="headerlink" href="#rdd" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>弹性分布式数据集（Resilient Distributed Datasets，RDD），是Spark中最基本的数据抽象，代表一个不可变、可分区的，元素可并行计算的集合</p>
<p><em>A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable,partitioned collection of elements that can be operated on in parallel</em></p>
<p>来源于Paper：Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory
Cluster Computing</p>
</div></blockquote>
<p>RDD提供了一个抽象的数据模型，不必担心底层数据的分布式特性，只需将具体的应用逻辑表
达为一系列转换操作（函数），不同RDD之间的转换操作之间还可以形成依赖关系，进而实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘IO和序列化开销，并且还提供了更多的API(map/reduec/filter/groupBy等等)</p>
<ul class="simple">
<li><p>RDD核心点</p>
<ul>
<li><p>不可变immutable</p></li>
<li><p>分区的partitioned</p></li>
<li><p>并行计算的parallel</p></li>
</ul>
</li>
</ul>
<div class="section" id="id1">
<h3><span class="section-number">11.1.1. </span>特性<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h3>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="nc">Internally</span><span class="o">,</span> <span class="n">each</span> <span class="nc">RDD</span> <span class="n">is</span> <span class="n">characterized</span> <span class="n">by</span> <span class="n">five</span> <span class="n">main</span> <span class="n">properties</span><span class="k">:</span>
 <span class="kt">-</span> <span class="kt">A</span> <span class="kt">list</span> <span class="kt">of</span> <span class="kt">partitions</span>
 <span class="o">-</span> <span class="n">A</span> <span class="n">function</span> <span class="k">for</span> <span class="n">computing</span> <span class="n">each</span> <span class="n">split</span>
 <span class="o">-</span> <span class="n">A</span> <span class="n">list</span> <span class="n">of</span> <span class="n">dependencies</span> <span class="n">on</span> <span class="n">other</span> <span class="nc">RDDs</span>
 <span class="o">-</span> <span class="nc">Optionally</span><span class="o">,</span> <span class="n">a</span> <span class="nc">Partitioner</span> <span class="k">for</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="nc">RDDs</span> <span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="n">to</span> <span class="n">say</span> <span class="n">that</span> <span class="n">the</span> <span class="nc">RDD</span> <span class="n">is</span> <span class="n">hash</span><span class="o">-</span><span class="n">partitioned</span><span class="o">)</span>
 <span class="o">-</span> <span class="nc">Optionally</span><span class="o">,</span> <span class="n">a</span> <span class="n">list</span> <span class="n">of</span> <span class="n">preferred</span> <span class="n">locations</span> <span class="n">to</span> <span class="n">compute</span> <span class="n">each</span> <span class="n">split</span> <span class="n">on</span> <span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="n">block</span> <span class="n">locations</span> <span class="k">for</span>
   <span class="n">an</span> <span class="nc">HDFS</span> <span class="n">file</span><span class="o">)</span>
</pre></div>
</div>
<p><img alt="RDD特性" src="../_images/RDDTrait.svg" /></p>
<p>RDD数据结构内部有五个特性</p>
<ul>
<li><p>A list of partitions</p>
<ul class="simple">
<li><p>一个分区（分片）列表，即数据集的基本组成单位</p></li>
<li><p>对于RDD来说，每个分片会被一个计算任务处理，分片决定并行度</p></li>
<li><p>用户可以在创建RDD的时候指定RDD分片数，没有指定则会采用默认值</p></li>
</ul>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span>  <span class="cm">/**</span>
<span class="cm">   * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span>
<span class="cm">   * be called once, so it is safe to implement a time-consuming computation in it.</span>
<span class="cm">   *</span>
<span class="cm">   * The partitions in this array must satisfy the following property:</span>
<span class="cm">   *   `rdd.partitions.zipWithIndex.forall { case (partition, index) =&gt; partition.index == index }`</span>
<span class="cm">   */</span>
  <span class="k">protected</span> <span class="k">def</span> <span class="n">getPartitions</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Partition</span><span class="o">]</span>
</pre></div>
</div>
</li>
<li><p>A function for computing each split</p>
<ul class="simple">
<li><p>Spark中的RDD计算是以分片为单位的，compute函数会被作用在每个分区上</p></li>
</ul>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="cm">/**</span>
<span class="cm">   * :: DeveloperApi ::</span>
<span class="cm">   * Implemented by subclasses to compute a given partition.</span>
<span class="cm">   */</span>
  <span class="nd">@DeveloperApi</span>
  <span class="k">def</span> <span class="n">compute</span><span class="o">(</span><span class="n">split</span><span class="k">:</span> <span class="kt">Partition</span><span class="o">,</span> <span class="n">context</span><span class="k">:</span> <span class="kt">TaskContext</span><span class="o">)</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span>
</pre></div>
</div>
</li>
<li><p>A list of dependencies on other RDDs</p>
<ul class="simple">
<li><p>RDD的每次转换都会生成一个新的RDD，故RDD之间就会形成类似流水线一样的<strong>前后依赖</strong>关系，在部分数据丢失时，就可以通过依赖关系重新计算丢失的分区数据，而不是对RDD所有的分区进行重新计算（容错机制）</p></li>
<li><p>Seq有序，也就体现了RDD的前后依赖</p></li>
</ul>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span>  <span class="cm">/**</span>
<span class="cm">   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span>
<span class="cm">   * be called once, so it is safe to implement a time-consuming computation in it.</span>
<span class="cm">   */</span>
  <span class="k">protected</span> <span class="k">def</span> <span class="n">getDependencies</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Dependency</span><span class="o">[</span><span class="k">_</span><span class="o">]]</span> <span class="o">=</span> <span class="n">deps</span>
</pre></div>
</div>
</li>
<li><p>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</p>
<ul class="simple">
<li><p>可选项，对于KeyValue类型的RDD会有一个Partitioner，即RDD的分区函数</p></li>
<li><p>当前Spark中实现了两种类型的分区函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。</p></li>
<li><p>只有对于key-value的RDD，才有Partitioner，非key-value的RDD，Parititioner值是None</p></li>
<li><p>Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量</p></li>
</ul>
</li>
<li><p>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</p>
<ul class="simple">
<li><p>可选项，一个列表，存储存取每个Partition的优先位置（prefer location）</p></li>
<li><p>对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置</p></li>
<li><p>按照”移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能选择那些存有数据的worker节点来进行任务计算。（本地优先计算）</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>RDD设计的一个重要优势是能够记录RDD间的依赖关系，也就是血统（lineage），通过丰富的转换操作（Transformation），可以构建一个复杂的DAG有向无环图，进而一步步计算</p>
</div></blockquote>
</div>
<div class="section" id="id2">
<h3><span class="section-number">11.1.2. </span>函数<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>主要有两类：Transformation和Action</p>
<p><em>Transformation操作只是建立计算关系，而Action 操作才是实际的执行者</em></p>
</div></blockquote>
<p><img alt="RDDFunctions" src="../_images/RDDFunctions.svg" /></p>
<ul>
<li><p>Transformation</p>
<ul>
<li><p>返回一个新的RDD</p>
<blockquote>
<div><p>which create a new dataset from an existing one</p>
</div></blockquote>
</li>
<li><p>所有Transformation函数都是<code class="docutils literal notranslate"><span class="pre">Lazy</span></code>，不会立即执行，需要Action函数触发</p></li>
</ul>
</li>
<li><p>Action</p>
<ul>
<li><p>返回值不是RDD（无返回值或者返回其他返回值）</p>
<blockquote>
<div><p>which return a value to the driver program after running a computation on the dataset</p>
</div></blockquote>
</li>
<li><p>所有Action函数立即执行（Eager），比如count、first、collect、take等</p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>注意：</p>
<ul class="simple">
<li><p>RDD不实际存储真正要计算的数据，而是记录了数据的位置在哪，数据的转换关系</p></li>
<li><p>RDD中的所有转换都是惰性求值/延迟执行的，也就是说并不会直接计算。只有当发生一个要求返回结果给Driver的Action动作时，这些转换才会真正运行。之所以使用惰性求值/延迟执行，是因为这样可以在Action时对RDD操作形成DAG有向无环图进行Stage的划分和并行优化，这种设计让Spark更加有效率地运行</p></li>
</ul>
</div></blockquote>
<div class="section" id="id3">
<h4><span class="section-number">11.1.2.1. </span>一些重要函数<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h4>
<p><img alt="RDDOperations" src="../_images/RDDOperations.svg" /></p>
</div>
</div>
</div>
<div class="section" id="id4">
<h2><span class="section-number">11.2. </span>持久化<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>在实际开发中某些RDD的计算或转换可能会比较耗费时间，如果这些RDD后续还会频繁的被使用到，那么可以将这些RDD进行持久化/缓存，这样下次再使用到的时候就不用再重新计算了，提高了程序运行的效率。</p>
</div></blockquote>
<div class="section" id="id5">
<h3><span class="section-number">11.2.1. </span>缓存函数<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span>  <span class="cm">/**</span>
<span class="cm">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span>
<span class="cm">   */</span>
  <span class="k">def</span> <span class="n">persist</span><span class="o">()</span><span class="k">:</span> <span class="kt">this.</span><span class="k">type</span> <span class="o">=</span> <span class="n">persist</span><span class="o">(</span><span class="nc">StorageLevel</span><span class="o">.</span><span class="nc">MEMORY_ONLY</span><span class="o">)</span>

  <span class="cm">/**</span>
<span class="cm">   * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span>
<span class="cm">   */</span>
  <span class="k">def</span> <span class="n">cache</span><span class="o">()</span><span class="k">:</span> <span class="kt">this.</span><span class="k">type</span> <span class="o">=</span> <span class="n">persist</span><span class="o">()</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h3><span class="section-number">11.2.2. </span>缓存级别<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">//缓存数据：选择缓存级别</span>
<span class="k">val</span> <span class="nc">NONE</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">DISK_ONLY</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">DISK_ONLY_2</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_ONLY</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_ONLY_2</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_ONLY_SER</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_ONLY_SER_2</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_AND_DISK</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_AND_DISK_2</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_AND_DISK_SER</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">MEMORY_AND_DISK_SER_2</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="mi">2</span><span class="o">)</span>
<span class="k">val</span> <span class="nc">OFF_HEAP</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StorageLevel</span><span class="o">(</span><span class="kc">true</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">true</span><span class="o">,</span> <span class="kc">false</span><span class="o">,</span> <span class="mi">1</span><span class="o">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h3><span class="section-number">11.2.3. </span>释放缓存<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>当缓存的RDD数据，不再被使用时，考虑释资源，使用如下函数：</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span>  <span class="cm">/**</span>
<span class="cm">   * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.</span>
<span class="cm">   *</span>
<span class="cm">   * @param blocking Whether to block until all blocks are deleted.</span>
<span class="cm">   * @return This RDD.</span>
<span class="cm">   */</span>
  <span class="k">def</span> <span class="n">unpersist</span><span class="o">(</span><span class="n">blocking</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">true</span><span class="o">)</span><span class="k">:</span> <span class="kt">this.</span><span class="k">type</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">logInfo</span><span class="o">(</span><span class="s">&quot;Removing RDD &quot;</span> <span class="o">+</span> <span class="n">id</span> <span class="o">+</span> <span class="s">&quot; from persistence list&quot;</span><span class="o">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">unpersistRDD</span><span class="o">(</span><span class="n">id</span><span class="o">,</span> <span class="n">blocking</span><span class="o">)</span>
    <span class="n">storageLevel</span> <span class="o">=</span> <span class="nc">StorageLevel</span><span class="o">.</span><span class="nc">NONE</span>
    <span class="k">this</span>
  <span class="o">}</span>
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h3><span class="section-number">11.2.4. </span>什么时候需要缓存数据？<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>当某个RDD被使用多次的时候，建议缓存此RDD数据</p></li>
<li><p>当某个RDD来之不易，并且使用不止一次，建议缓存此RDD数据</p></li>
</ul>
</div>
</div>
<div class="section" id="checkpoint">
<h2><span class="section-number">11.3. </span>Checkpoint<a class="headerlink" href="#checkpoint" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>RDD 数据可以持久化，但是持久化/缓存可以把数据放在内存中，虽然是快速的，但是也是最
不可靠的；也可以把数据放在磁盘上，也不是完全可靠的！例如磁盘会损坏等。</p>
<p>Checkpoint的产生就是为了更加可靠的数据持久化，在Checkpoint的时候一般把数据放在在
HDFS上，这就天然的借助了HDFS天生的高容错、高可靠来实现数据最大程度上的安全，实现了RDD
的容错和高可用。</p>
<p>在Spark Core中对RDD做checkpoint，可以切断做checkpoint RDD的依赖关系，将RDD数据
保存到可靠存储（如HDFS）以便数据恢复</p>
</div></blockquote>
<ul class="simple">
<li><p>持久化和Checkpoint的区别</p>
<ul>
<li><p>存储位置不同</p>
<ul>
<li><p>Persist 和 Cache 只能保存在本地的磁盘和内存中(或者堆外内存)；</p></li>
<li><p>Checkpoint 可以保存数据到 HDFS 这类可靠的存储上；</p></li>
</ul>
</li>
<li><p>生命周期</p>
<ul>
<li><p>Cache和Persist的RDD会在程序结束后会被清除或者手动调用unpersist方法；</p></li>
<li><p>Checkpoint的RDD在程序结束后依然存在，不会被删除</p></li>
</ul>
</li>
<li><p>Lineage(血统、依赖链、依赖关系)</p>
<ul>
<li><p>Persist和Cache，不会丢掉RDD间的依赖链/依赖关系，因为这种缓存是不可靠的，如果出现了一些错误(例如 Executor 宕机)，需要通过回溯依赖链重新计算出来</p></li>
<li><p>Checkpoint会斩断依赖链，因为Checkpoint会把结果保存在HDFS这类存储中，更加的安
全可靠，一般不需要回溯依赖链</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id9">
<h2><span class="section-number">11.4. </span>共享变量<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h2>
<p>在默认情况下，当Spark在集群的多个不同节点的多个任务上并行运行一个函数时，它会把函数中涉及到的每个变量，在每个任务上都生成一个副本。但是，有时候需要在多个任务之间共享变量，或者在任务(Task)和任务控制节点(Driver Program)之间共享变量。</p>
<ul class="simple">
<li><p>Spark提供了两种类型的变量：</p>
<ul>
<li><p>广播变量Broadcast Variables</p>
<ul>
<li><p>广播变量用来把变量在所有节点的内存之间进行共享，在每个机器上缓存一个只读的变
量，而不是为机器上的每个任务都生成一个副本</p></li>
</ul>
</li>
<li><p>累加器Accumulators</p>
<ul>
<li><p>累加器支持在所有不同节点之间进行累加计算(比如计数或者求和)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="section" id="id10">
<h3><span class="section-number">11.4.1. </span>广播变量<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h3>
<p>广播变量允许开发人员在每个节点（Worker or Executor）缓存只读变量，而不是在Task之间传递这些变量。使用广播变量能够高效地在集群每个节点创建大数据集的副本。同时Spark还使用高效的广播算法分发这些变量，从而减少通信的开销。</p>
<p><img alt="BroadcastVariable" src="../_images/BroadcastVariable-1606138193037.svg" /></p>
</div>
<div class="section" id="id11">
<h3><span class="section-number">11.4.2. </span>累加器<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
<p>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，即确提供了多个task对一个变量并行操作的功能。但是task只能对Accumulator进行累加操作，不能读取Accumulator的值，只有Driver程序可以读取Accumulator的值。创建的Accumulator变量的值能够在Spark Web UI上看到，在创建时应该尽量为其命名。</p>
<p><img alt="image-20201123213330682" src="../_images/image-20201123213330682.png" /></p>
<p><strong>Spark内置了三种类型的Accumulator，分别是<code class="docutils literal notranslate"><span class="pre">LongAccumulator</span></code>用来累加整数型，<code class="docutils literal notranslate"><span class="pre">DoubleAccumulator</span></code>用来累加浮点型，<code class="docutils literal notranslate"><span class="pre">CollectionAccumulator</span></code>用来累加集合元素。</strong></p>
</div>
</div>
<div class="section" id="spark">
<h2><span class="section-number">11.5. </span>Spark内核调度<a class="headerlink" href="#spark" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Spark的核心是根据RDD来实现的，Spark Scheduler则为Spark核心实现的重要一环，其作用就是任务调度。Spark的任务调度就是如何组织任务去处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG，基于DAG划分Stage，将每个Stage中的任务发到指定节点运行。基于Spark的任务调度原理，可以合理规划资源利用，做到尽可能用最少的资源高效地完成任务计算。</p>
</div></blockquote>
<p>以词频统计WordCount程序为例，Job执行是DAG图：</p>
<p><img alt="image-20201123213604857" src="../_images/image-20201123213604857.png" /></p>
<div class="section" id="id12">
<h3><span class="section-number">11.5.1. </span>RDD依赖<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h3>
<p>RDD 的容错机制是通过将 RDD 间转移操作构建成有向无环图来实现的。从抽象的角度看，RDD 间存在着血统继承关系，其本质上是 RDD之间的依赖（Dependency）关系。
从图的角度看，RDD 为节点，在一次转换操作中，创建得到的新 RDD 称为子 RDD，同时会产生新的边，即依赖关系，子 RDD 依赖向上依赖的 RDD 便是父 RDD，可能会存在多个父 RDD。</p>
<p>可以将这种依赖关系进一步分为两类，分别是<code class="docutils literal notranslate"><span class="pre">窄依赖（NarrowDependency）</span></code>和 <code class="docutils literal notranslate"><span class="pre">Shuffle</span> <span class="pre">依赖</span></code>（<code class="docutils literal notranslate"><span class="pre">Shuffle</span> <span class="pre">Dependency</span></code> 在部分文献中也被称为 <code class="docutils literal notranslate"><span class="pre">Wide</span> <span class="pre">Dependency</span></code>，即宽依赖）。</p>
<div class="section" id="id13">
<h4><span class="section-number">11.5.1.1. </span>窄依赖<a class="headerlink" href="#id13" title="永久链接至标题">¶</a></h4>
<blockquote>
<div><p>父 RDD 与子 RDD 间的分区是一对一的</p>
</div></blockquote>
</div>
<div class="section" id="id14">
<h4><span class="section-number">11.5.1.2. </span>宽依赖<a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h4>
<blockquote>
<div><p>父 RDD 中的分区可能会被多个子 RDD 分区使用</p>
<p>因为父 RDD 中一个分区内的数据会被分割并发送给子 RDD 的所有分区，因此 Shuffle 依赖也意味着父 RDD与子 RDD 之间存在着 Shuffle 过程。</p>
</div></blockquote>
</div>
<div class="section" id="id15">
<h4><span class="section-number">11.5.1.3. </span>为什么要设计宽窄依赖？<a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h4>
<ul class="simple">
<li><p>对于窄依赖来说：Spark可以并行计算；如果有一个分区数据丢失，只需要从父RDD的对应
个分区重新计算即可，不需要重新计算整个任务，提高容错</p></li>
<li><p>对应宽依赖来说：划分Stage的依据，产生Shuffle</p></li>
</ul>
</div>
</div>
<div class="section" id="stagedag">
<h3><span class="section-number">11.5.2. </span>Stage和DAG<a class="headerlink" href="#stagedag" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>DAG</p>
<p>如果一个有向图无法从任意顶点出发经过若干条边回到该点，则这个图是一个有向无环图（DAG图）。而在Spark中，由于计算过程很多时候会有先后顺序，受制于某些任务必须比另一些任务较早执行的限制，必须对任务进行排队，形成一个队列的任务集合，这个队列的任务集合就是DAG图，每一个定点就是一个任务，每一条边代表一种限制约束（Spark中的依赖关系）</p>
</div></blockquote>
<p><img alt="image-20201123215014643" src="../_images/image-20201123215014643.png" /></p>
<blockquote>
<div><p>从上图可以看出，Stage0和Stage1被一条S形线相连，S形线代表产生了Shuffle</p>
<p>Spark中DAG生成过程的重点是对Stage的划分，其划分的依据是RDD的依赖关系，对于不同
的依赖关系，高层调度器会进行不同的处理。</p>
</div></blockquote>
<ul class="simple">
<li><p>对于窄依赖，RDD之间的数据不需要进行Shuffle，多个数据处理可以在同一台机器的内存中完
成，所以窄依赖在Spark中被划分为同一个Stage</p></li>
<li><p>对于宽依赖，由于Shuffle的存在，必须等到父RDD的Shuffle处理完成后，才能开始接下来的计
算，所以会在此处进行Stage的切分</p></li>
</ul>
<p><strong>把DAG划分成互相依赖的多个Stage，划分依据是RDD之间的宽依赖，Stage是由一组并行的Task组成。</strong></p>
<p>Stage切割规则：从后往前，遇到宽依赖就切割Stage。</p>
<p><strong>Stage计算模式：pipeline管道计算模式</strong>，pipeline只是一种计算思想、模式，来一条数据然后计算一条数据，把所有的逻辑走完，然后落地。准确的说：一个task处理一串分区的数据，整个计算逻辑全部走完。</p>
</div>
</div>
<div class="section" id="spark-shuffle">
<h2><span class="section-number">11.6. </span>Spark Shuffle<a class="headerlink" href="#spark-shuffle" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Spark在DAG调度阶段会将一个Job划分为多个Stage，上游Stage做map工作，下游Stage做reduce工作，其本质上还是MapReduce计算框架。Shuffle是连接map和reduce之间的桥梁，它将map的输出对应到reduce输入中，涉及到序列化反序列化、跨节点网络IO以及磁盘读写IO等。</p>
</div></blockquote>
<p><img alt="image-20201123215757613" src="../_images/image-20201123215757613.png" /></p>
<p>Spark的Shuffle分为Write和Read两个阶段，分属于两个不同的Stage，前者是Parent Stage的最后一步，后者是Child Stage的第一步</p>
<p><img alt="image-20201123215832736" src="../_images/image-20201123215832736.png" /></p>
<ul class="simple">
<li><p>执行Shuffle的主体是Stage中的并发任务，这些任务分ShuffleMapTask和ResultTask两种，ShuffleMapTask要进行Shuffle，ResultTask负责返回计算结果，一个Job中只有最后的Stage采用ResultTask，其他的均为ShuffleMapTask。</p></li>
<li><p>如果要按照map端和reduce端来分析的话，ShuffleMapTask可以即是map端任务，又是reduce端任务，因为Spark中的Shuffle是可以串行的；ResultTask则只能充当reduce端任务的角色</p></li>
</ul>
</div>
<div class="section" id="job">
<h2><span class="section-number">11.7. </span>Job调度流程<a class="headerlink" href="#job" title="永久链接至标题">¶</a></h2>
<p>当启动Spark Application的时候，运行MAIN函数，首先创建SparkContext对象（构建DAGScheduler和TaskScheduler）</p>
<ul class="simple">
<li><p>DAGScheduler实例对象</p>
<ul>
<li><p>将每个Job的DAG图划分为Stage，依据RDD之间依赖为宽依赖（产生Shuffle）</p></li>
</ul>
</li>
<li><p>TaskScheduler实例对象</p>
<ul>
<li><p>调度每个Stage中所有Task：TaskSet，发送到Executor上执行</p></li>
</ul>
</li>
</ul>
<p><img alt="JobSchedule" src="../_images/JobSchedule.svg" /></p>
<p>Spark RDD通过其Transactions操作，形成了RDD血缘关系图，即DAG，最后通过Action的调用，触发Job并调度执行</p>
<ul class="simple">
<li><p>DAGScheduler负责Stage级的调度，主要是将DAG切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度</p></li>
<li><p>TaskScheduler负责Task级的调度，将DAGScheduler给过来的TaskSet按照指定的调度策略分发到Executor上执行，调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多种实现，分别对接不同的资源管理系统</p></li>
</ul>
<p>一个Spark应用程序包括Job、Stage及Task</p>
<ul class="simple">
<li><p>Job是以Action方法为界，遇到一个Action方法则触发一个Job</p></li>
<li><p>Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次划分</p></li>
<li><p>Task是Stage的子集，以并行度(分区数)来衡量，分区数是多少，则有多少个task</p></li>
</ul>
</div>
<div class="section" id="id16">
<h2><span class="section-number">11.8. </span>并行度<a class="headerlink" href="#id16" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Spark作业中，各个stage的task数量，代表了Spark作业在各个阶段stage的并行度</p>
</div></blockquote>
<ul class="simple">
<li><p>在Spark Application运行时，并行度可以从两个方面理解：</p>
<ul>
<li><p>资源的并行度：由节点数(executor)和cpu数(core)决定的</p></li>
<li><p>数据的并行度：task的数据，partition大小</p>
<ul>
<li><p>task又分为map时的task和reduce(shuffle)时的task</p></li>
<li><p>task的数目和很多因素有关，资源的总core数，spark.default.parallelism参数，spark.sql.shuffle.partitions参数，读取数据源的类型,shuffle方法的第二个参数,repartition的数目等等</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<div><p>如果Task的数量多，能用的资源也多，那么并行度自然就好。如果Task的数据少，资源很多，有一定的浪费，但是也还好。</p>
<p>如果Task数目很多，但是资源少，那么会执行完一批，再执行下一批。所以官方给出的建议是，这个Task数目要是core总数的<strong>2-3倍</strong>为佳。如果core有多少Task就有多少，那么有些比较快的task执行完了，一些资源就会处于等待的状态。</p>
</div></blockquote>
<div class="section" id="task">
<h3><span class="section-number">11.8.1. </span>设置Task数量<a class="headerlink" href="#task" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>将Task数量设置成与Application总CPU Core 数量相同（理想情况，150个core，分配150 Task）</p></li>
<li><p>官方推荐，<strong>Task数量，设置成Application总CPU Core数量的2~3倍</strong>（150个cpu core，设置task数量300~500）</p></li>
<li><p>与理想情况不同的是：有些Task会运行快一点，比如50s就完了，有些Task可能会慢一点，要一分半才运行完，所以如果你的Task数量，刚好设置的跟CPU Core数量相同，也可能会导致资源的浪费，比如150 Task，10个先运行完了，剩余140个还在运行，但是这个时候，就有10个CPU Core空闲出来了，导致浪费。如果设置2~3倍，那么一个Task运行完以后，另外一个Task马上补上来，尽量让CPU Core不要空闲。</p></li>
</ul>
</div>
<div class="section" id="id17">
<h3><span class="section-number">11.8.2. </span>设置并行度<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h3>
<p><img alt="image-20201123222508399" src="../_images/image-20201123222508399.png" /></p>
<p>Task没有设置或者设置的很少，比如为100个task ，平均分配一下，每个executor 分配到2个task，每个executor 剩下的一个cpu core 就浪费了！</p>
<p>并行度没有与资源相匹配，导致你分配下去的资源都浪费掉了。**合理的并行度的设置，应该要设置的足够大，大到可以完全合理的利用你的集群资源。**可以调整Task数目，按照原则：Task数量，设置成Application总CPU Core数量的2~3倍</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="SparkSQL.html" class="btn btn-neutral float-right" title="12. Spark SQL" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Spark.html" class="btn btn-neutral float-left" title="10. Spark" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; 版权所有 2020, roohom

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>