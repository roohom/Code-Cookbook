

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>13. Spark SQL &mdash; Code-Cookbook 0.1 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="14. Spark Streaming" href="SparkStreaming.html" />
    <link rel="prev" title="12. Spark Core" href="SparkCore.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Code-Cookbook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">大数据</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Bigdata Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Apache%20Druid.html">1. Apache Druid</a></li>
<li class="toctree-l2"><a class="reference internal" href="Apache%20Flume.html">2. Apache Flume</a></li>
<li class="toctree-l2"><a class="reference internal" href="Flink.html">3. Flink</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hadoop%E6%90%AD%E5%BB%BA%E6%80%BB%E4%BD%93%E6%AD%A5%E9%AA%A4.html">4. Hadoop搭建总体步骤</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hbase.html">5. Hbase</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hive%E6%95%B0%E4%BB%93.html">6. Hive</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kafka.html">7. Kafka</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kudu.html">8. Kudu</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kylin.html">9. Kylin</a></li>
<li class="toctree-l2"><a class="reference internal" href="Redis.html">10. Redis</a></li>
<li class="toctree-l2"><a class="reference internal" href="Spark.html">11. Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="SparkCore.html">12. Spark Core</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">13. Spark SQL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">13.1. 概述</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">13.1.1. 历史时间线</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">13.1.2. 数据结构</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dataframe">13.2. DataFrame</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#rdddataframe">13.2.1. RDD与DataFrame的区别</a></li>
<li class="toctree-l4"><a class="reference internal" href="#shuffle">13.2.2. Shuffle分区数目</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dataset">13.3. Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rdddataframedataset">13.4. RDD、DataFrame和Dataset之间的转化关系</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">13.4.1. 转换关系</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">13.4.2. RDD转DataFrame的方式及原因</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">13.4.3. 理解说明</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id7">13.5. 如何理解RDD、DataFrame和Dataset</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="SparkStreaming.html">14. Spark Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="StructuredStreaming.html">15. Structured Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Zookeeper.html">16. Zookeeper</a></li>
<li class="toctree-l2"><a class="reference internal" href="ZookeeperAndHadoop.html">17. ZookeeperAndHadoop</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BD%AF%E4%BB%B6%E5%90%AF%E5%8A%A8%E6%8C%87%E5%8D%97.html">18. 常用软件梳理</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">博客</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Blog%20Here/index.html">Blogs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Bigdata Tools</a> &raquo;</li>
        
      <li><span class="section-number">13. </span>Spark SQL</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/Bigdata Tools/SparkSQL.md.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark-sql">
<h1><span class="section-number">13. </span>Spark SQL<a class="headerlink" href="#spark-sql" title="永久链接至标题">¶</a></h1>
<div class="section" id="id1">
<h2><span class="section-number">13.1. </span>概述<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p><strong>Spark SQL</strong> is Apache Spark’s module for working with structured data.</p>
<p><em>针对结构化数据处理的模块</em></p>
</div></blockquote>
<p>从Spark 2.2.0开始，在生产环境中既可以用于离线分析又可以用于实时分析</p>
<ul class="simple">
<li><p>离线分析：<code class="docutils literal notranslate"><span class="pre">SparkSQL</span></code>模块</p></li>
<li><p>实时分析：<code class="docutils literal notranslate"><span class="pre">StructuredStreaming</span></code>模块，属于SparkSQL模块针对实时流式数据处理功能</p></li>
</ul>
<div class="section" id="id2">
<h3><span class="section-number">13.1.1. </span>历史时间线<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>SparkSQL模块一直到Spark 2.0版本才算真正稳定，发挥其巨大功能</p>
</div></blockquote>
<p><img alt="image-20201122100609282" src="../_images/image-20201122100609282.png" /></p>
<p>SparkSQL模块来源于Hive框架，但是其功能远大于Hive，SparkSQL用于Hive的所有功能，并且SparkSQL天然集成（兼容）Hive，从其中读取数据进行分析处理</p>
</div>
<div class="section" id="id3">
<h3><span class="section-number">13.1.2. </span>数据结构<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>$$
DataFrame = RDD[Row] + Schema
$$</p>
<p>$$
Dataset =  RDD[case classes] + Schema
$$</p>
<p>$$
DataFrame = Dataset[Row]
$$</p>
<blockquote>
<div><p>DataFrame = RDD[Row] + Schema</p>
</div></blockquote>
<blockquote>
<div><p>Dataset =  RDD[case classes] + Schema</p>
</div></blockquote>
<blockquote>
<div><p>DataFrame = Dataset[Row]</p>
</div></blockquote>
</div>
</div>
<div class="section" id="dataframe">
<h2><span class="section-number">13.2. </span>DataFrame<a class="headerlink" href="#dataframe" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>在Spark中，<strong>DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格</strong></p>
<p>DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行针对性的优化，最终达到大幅提升运行时效率。反观RDD，由于无从得
知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。</p>
</div></blockquote>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span></code>： <em>A DataFrame is a DataSet organized into named columns</em>.（以列（列名，列类
型，列值）的形式构成的分布式的数据集，按照列赋予不同的名称）</p>
<p><img alt="image-20201122142741172" src="../_images/image-20201122142741172.png" /></p>
<ul class="simple">
<li><p>特性：</p>
<ul>
<li><p>1）、分布式的数据集，并且以列的方式组合的，相当于具有schema的RDD；</p></li>
<li><p>2）、相当于关系型数据库中的表，但是底层有优化；</p></li>
<li><p>3）、提供了一些抽象的操作，如select、filter、aggregation、plot；</p></li>
<li><p>4）、它是由于R语言或者Pandas语言处理小数据集的经验应用到处理分布式大数据集上；</p></li>
<li><p>5）、在1.3版本之前，叫SchemaRDD；</p></li>
</ul>
</li>
</ul>
<div class="section" id="rdddataframe">
<h3><span class="section-number">13.2.1. </span>RDD与DataFrame的区别<a class="headerlink" href="#rdddataframe" title="永久链接至标题">¶</a></h3>
<p>DataFrame与RDD的主要区别在于，DataFrame带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。</p>
<p>RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。 DataFrame底层是以RDD为基础的分布式数据集，和RDD的主要区别的是：RDD中没有schema信息，而DataFrame中数据每一行都包含schema</p>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span> <span class="pre">=</span> <span class="pre">RDD[Row]</span> <span class="pre">+</span> <span class="pre">shcema</span></code></p>
</div>
<div class="section" id="shuffle">
<h3><span class="section-number">13.2.2. </span>Shuffle分区数目<a class="headerlink" href="#shuffle" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>在SparkSQL中当Job中产生Shuffle时，默认的分区数（spark.sql.shuffle.partitions ）为200，在实际项目中要合理的设置。在构建SparkSession实例对象时，设置参数的值：</p>
</div></blockquote>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="c1">// 构建SparkSession实例对象</span>
<span class="kd">val</span> <span class="n">spark</span><span class="p">:</span> <span class="nc">SparkSession</span> <span class="o">=</span> <span class="nc">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">()</span>
	<span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">&quot;local[4]&quot;</span><span class="p">)</span>
	<span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="bp">this</span><span class="p">.</span><span class="n">getClass</span><span class="p">.</span><span class="n">getSimpleName</span><span class="p">.</span><span class="n">stripSuffix</span><span class="p">(</span><span class="s">&quot;$&quot;</span><span class="p">))</span>
	<span class="c1">// TODO: 设置shuffle时分区数目</span>
	<span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="p">,</span> <span class="s">&quot;4&quot;</span><span class="p">)</span>
	<span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
	<span class="c1">// 导入隐式转换</span>
<span class="k">import</span> <span class="nn">spark</span><span class="p">.</span><span class="nn">implicits</span><span class="p">.</span><span class="n">_</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="dataset">
<h2><span class="section-number">13.3. </span>Dataset<a class="headerlink" href="#dataset" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code>：<em>A DataSet is a distributed collection of data.</em> (分布式的数据集)</p>
</div></blockquote>
<p>Dataset是一个强类型的特定领域的对象，这种对象可以函数式或者关系操作并行地转换.</p>
<p>从Spark 2.0开始，DataFrame与Dataset合并，每个Dataset也有一个被称为一个DataFrame的
类型化视图，这种DataFrame是Row类型的Dataset，即Dataset[Row]。Dataset API是DataFrames的扩展，它提供了一种类型安全的，面向对象的编程接口。它是一个强类型，不可变的对象集合，映射到关系模式。在数据集的核心 API是一个称为编码器的新概念，它负责在JVM对象和表格表示之间进行转换。表格表示使用Spark内部Tungsten二进制格式存储，允许对序列化数据进行操作并提高内存利用率。</p>
<blockquote>
<div><p>Dataset =  RDD[case class] + Schema</p>
</div></blockquote>
</div>
<div class="section" id="rdddataframedataset">
<h2><span class="section-number">13.4. </span>RDD、DataFrame和Dataset之间的转化关系<a class="headerlink" href="#rdddataframedataset" title="永久链接至标题">¶</a></h2>
<p><img alt="RDD_DataFrame_DatasetTransformation" src="../_images/RDD_DataFrame_DatasetTransformation.svg" /></p>
<p><img alt="SparkSQLStruct" src="../_images/SparkSQLStruct.svg" /></p>
<div class="section" id="id4">
<h3><span class="section-number">13.4.1. </span>转换关系<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span> <span class="pre">=</span> <span class="pre">RDD[Row]+Schema</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">Dataset</span> <span class="pre">=</span> <span class="pre">RDD[case</span> <span class="pre">clase]</span> <span class="pre">+</span> <span class="pre">Schema</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span> <span class="pre">=</span> <span class="pre">Dataset[Row]</span></code></p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">13.4.2. </span>RDD转DataFrame的方式及原因<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<p>将RDD转化为DataFrame有两种方式:</p>
<ul>
<li><p>方式一：通过反射推断schema 要求：RDD的元素类型必须是case class</p>
<p><img alt="image-20201123203015524" src="../_images/image-20201123203015524.png" /></p>
</li>
<li><p>方式二、编程指定schema 要求：RDD的元素类型必须是Row 自己编写schema（StructType） 调用SparkSession的createDatafrmame（RDD[Row],schema）</p>
<p><img alt="image-20201123203607694" src="../_images/image-20201123203607694.png" /></p>
</li>
</ul>
</div>
<div class="section" id="id6">
<h3><span class="section-number">13.4.3. </span>理解说明<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<ul class="simple">
<li><p>RDD是弹性分布式数据集</p></li>
<li><p>DataFrame类似于二维表格</p></li>
<li><p>Row封装的是数据结果，就像其名字row一样，是一行数据，但是不清楚到底是什么数据，数据是什么结构，仅仅是一行数据而已</p></li>
<li><p>当Dataset里面装入的是不清楚结构的一些行数据，并且带有Schema信息，就成了DataFrame，这也就是说Dataset是强类型的，<strong>Dataset是强类型的数据安全的</strong>。</p></li>
<li><p>在存储上，Dataset里的数据按照特定的形式存储，节省存储空间。</p></li>
</ul>
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">13.5. </span>如何理解RDD、DataFrame和Dataset<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>一、数据结构RDD：</p>
<ul>
<li><p>DD（Resilient Distributed Datasets）叫做弹性分布式数据集，是Spark中最基本的数据
抽象，源码中是一个抽象类，代表一个不可变、可分区、里面的元素可并行计算的集合。</p></li>
<li><p>编译时类型安全，但是无论是集群间的通信，还是IO操作都需要对对象的结构和数据进行
序列化和反序列化，还存在较大的GC的性能开销，会频繁的创建和销毁对象。</p></li>
</ul>
</li>
<li><p>二、数据结构DataFrame：</p>
<ul>
<li><p>与RDD类似，DataFrame是一个分布式数据容器，不过它<strong>更像数据库中的二维表格</strong>，除了
数据之外，还记录这数据的结构信息（即schema）。</p></li>
<li><p>DataFrame也是懒执行的，性能上要比RDD高（主要因为执行计划得到了优化）。</p></li>
<li><p>由于DataFrame每一行的数据结构一样，且存在schema中，Spark通过schema就能读懂
数据，因此在通信和IO时只需要序列化和反序列化数据，而结构部分不用。</p></li>
<li><p>Spark能够以二进制的形式序列化数据到JVM堆以外（off-heap：非堆）的内存，这些内
存直接受操作系统管理，也就不再受JVM的限制和GC的困扰了。但是DataFrame不是类
型安全的。</p></li>
</ul>
</li>
<li><p>三、数据结构Dataset：</p>
<ul>
<li><p>Dataset是DataFrame API的一个扩展，是Spark最新的数据抽象，结合了RDD和DataFrame
的优点。</p></li>
<li><p>DataFrame=Dataset[Row]（Row表示表结构信息的类型），<strong>DataFrame只知道字段，但</strong>
<strong>是不知道字段类型，而Dataset是强类型的，不仅仅知道字段，而且知道字段类型</strong>。</p></li>
<li><p><strong>样例类CaseClass被用来在Dataset中定义数据的结构信息，样例类中的每个属性名称直接</strong>
<strong>对应到Dataset中的字段名称。</strong></p></li>
<li><p>Dataset具有类型安全检查，也具有DataFrame的查询优化特性，还支持编解码器，当需
要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</p></li>
</ul>
</li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="SparkStreaming.html" class="btn btn-neutral float-right" title="14. Spark Streaming" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="SparkCore.html" class="btn btn-neutral float-left" title="12. Spark Core" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2020-2021, roohom.

    </p>
  </div>
    
    
    
    利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    
    由 <a href="https://readthedocs.org">Read the Docs</a>开发. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>