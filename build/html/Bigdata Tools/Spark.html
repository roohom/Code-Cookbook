

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>10. Spark &mdash; Code-Cookbook 0.1 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/translations.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="11. Spark Core" href="SparkCore.html" />
    <link rel="prev" title="9. Redis" href="Redis.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Code-Cookbook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">大数据</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Bigdata/index.html">Bigdata</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Bigdata Tools</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Apache%20Druid.html">1. Apache Druid</a></li>
<li class="toctree-l2"><a class="reference internal" href="Apache%20Flume.html">2. Apache Flume</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bigdata%E9%98%B2%E6%87%B5%E9%80%BC%E6%8C%87%E5%8D%97.html">3. Bigdata Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="Flink%E5%9F%BA%E7%A1%80%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86.html">4. Flink基础配置与基础原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hadoop%E6%90%AD%E5%BB%BA%E6%80%BB%E4%BD%93%E6%AD%A5%E9%AA%A4.html">5. Hadoop搭建总体步骤</a></li>
<li class="toctree-l2"><a class="reference internal" href="Hbase.html">6. Hbase</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kafka.html">7. Kafka</a></li>
<li class="toctree-l2"><a class="reference internal" href="Kylin.html">8. Kylin</a></li>
<li class="toctree-l2"><a class="reference internal" href="Redis.html">9. Redis</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">10. Spark</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">10.1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">10.2. 框架模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">10.3. 运行模式</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quick-start">10.4. Quick Start</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">10.5. Spark应用组成</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-shell">10.6. 使用Spark Shell</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spark-standalone">10.7. Spark Standalone集群</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">10.7.1. 架构</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">10.7.2. 搭建步骤</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">10.7.3. 启动服务</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark-submit">10.8. 提交程序运行Spark Submit</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id7">10.8.1. 示例</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">10.8.2. 提交参数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#executor">10.8.3. Executor 参数配置</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id10">10.8.4. 官方案例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark-standalone-ha">10.9. Spark Standalone  HA</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id11">10.9.1. 搭建配置</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">10.9.2. 测试运行</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spark-on-yarn">10.10. Spark On Yarn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id13">10.10.1. 搭建步骤</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id24">10.10.2. 启动服务</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-mode">10.11. Deploy Mode</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#client">10.11.1. Client模式</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cluster">10.11.2. <strong>Cluster模式</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="SparkCore.html">11. Spark Core</a></li>
<li class="toctree-l2"><a class="reference internal" href="SparkSQL.html">12. Spark SQL</a></li>
<li class="toctree-l2"><a class="reference internal" href="SparkStreaming.html">13. Spark Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="StructuredStreaming.html">14. Structured Streaming</a></li>
<li class="toctree-l2"><a class="reference internal" href="Zookeeper.html">15. Zookeeper</a></li>
<li class="toctree-l2"><a class="reference internal" href="%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BD%AF%E4%BB%B6%E5%90%AF%E5%8A%A8%E6%8C%87%E5%8D%97.html">16. 常用软件梳理</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">博客</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Blogs/index.html">Blogs</a></li>
</ul>
<p class="caption"><span class="caption-text">大数据辅助工具</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Auxiliary%20tools/index.html">Auxiliary tools</a></li>
</ul>
<p class="caption"><span class="caption-text">SQL相关</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../SQL/index.html">SQL</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Code-Cookbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Bigdata Tools</a> &raquo;</li>
        
      <li><span class="section-number">10. </span>Spark</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/Bigdata Tools/Spark.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark">
<h1><span class="section-number">10. </span>Spark<a class="headerlink" href="#spark" title="永久链接至标题">¶</a></h1>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">Apache</span> <span class="pre">Spark</span> <span class="pre">is</span> <span class="pre">a</span> <span class="pre">unified</span> <span class="pre">analytics</span> <span class="pre">engine</span> <span class="pre">for</span> <span class="pre">large-scale</span> <span class="pre">data</span> <span class="pre">processing.</span></code></p>
</div></blockquote>
<blockquote>
<div><p>起源于加州大学伯克利分校，来源于一篇论文<a class="reference external" href="#">弹性分布式数据集(<code class="docutils literal notranslate"><span class="pre">RDD</span></code>)</a></p>
<p>1、分析引擎：类似MapReduce框架，分析海量数据</p>
<p>2、统一分析引擎：针对任意数据分析需求，都可以分析数据</p>
<p>3、大规模数据分析引擎：处理数据可以海量数据</p>
</div></blockquote>
<div class="section" id="introduction">
<h2><span class="section-number">10.1. </span>Introduction<a class="headerlink" href="#introduction" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RDD（弹性分布式数据集）</span></code>是Spark框架的核心概念，属于分布式集合，将数据划分到不同的partition中，每个分区数据被一个Task任务处理（类似MapReduce框架中处理一个Block数据使用一个MapTask任务）</p></li>
</ul>
<p><img alt="1605666594709" src="../_images/1605666594709.png" /></p>
<ul>
<li><p>Spark与MapReduce框架的比较</p>
<p><img alt="image-20201118104414877" src="../_images/image-20201118104414877.png" /></p>
</li>
<li><p>Spark框架仅仅是一个分析数据的框架</p>
<ul class="simple">
<li><p>数据来源：支持任何数据源，能够从任意存储引擎读写数据</p></li>
<li><p>应用程序运行：本地模式、集群模式(Standalone、Hadoop YARN、容器)</p></li>
</ul>
</li>
<li><p>Spark应用程序运行模式：本地和集群、云端</p>
<ul class="simple">
<li><p><a class="reference external" href="#">Spark框架将数据封装在RDD集合中，每个集合RDD由多个分区Partition组成，每个分区Partition数据被一个Task处理，每个Task任务启动一个线程Thread，每个Task运行需要一核CPU</a></p></li>
</ul>
</li>
</ul>
<blockquote>
<div><p>Spark程序运行本地模式，可以指定JVM启动几个线程Thread或者分配多少Core CPU</p>
</div></blockquote>
</div>
<div class="section" id="id1">
<h2><span class="section-number">10.2. </span>框架模块<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p><img alt="image-20201118164922445" src="../_images/image-20201118164922445.png" /></p>
<ul>
<li><p>Spark Core</p>
<p>实现了Spark的基本功能，包含RDD、任务调度、内存管理、错误恢复、与存储系统交互等模块。</p>
<p>数据结构<code class="docutils literal notranslate"><span class="pre">RDD</span></code></p>
</li>
<li><p>Spark SQL</p>
<p>用来操作结构化数据的程序包，通过Spark SQL，可以使用SQL操作数据。</p>
<p>数据结构：DataSet/DataFrame = RDD + Schema</p>
</li>
<li><p>Spark Streaming</p>
<p>Spark提供的堆实时数据进行流式计算的组件，提供了用来操作数据流的API。</p>
<p>数据结构：DStream = Seq[EDD]</p>
</li>
<li><p>Spark MLlib</p>
<p>提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能。</p>
<p>数据结构：RDD或者DataFrame</p>
</li>
<li><p>Spark GraphX</p>
<p>Spark中用于图计算的API，性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。</p>
<p>数据结构：RDD或者DataFrame</p>
</li>
<li><p>Structured Streaming</p>
<p>Structured Streaming结构化流处理模块，将流式结构化数据封装到DataFrame中进行分析。</p>
</li>
</ul>
<p><img alt="image-20201118165531399" src="../_images/image-20201118165531399.png" /></p>
<p>Structured Streaming是建立在SparkSQL引擎之上的可伸缩和高容错的流式处理引擎，可以像操作静态数据的批量计算一样来执行流式计算。当流式数据不断的到达的过程中Spark SQL的引擎会连续不断的执行计算并更新最终结果。</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">10.3. </span>运行模式<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<ul class="simple">
<li><p>Spark 框架编写的应用程序可以运行在本地模式（Local Mode）、集群模式（Cluster Mode）和云服务（Cloud），方便开发测试和生产部署。</p></li>
</ul>
<p><img alt="image-20201118175804243" src="../_images/image-20201118175804243.png" /></p>
<ul class="simple">
<li><p>本地模式：Local Mode</p>
<ul>
<li><p>将Spark 应用程序中任务Task运行在一个本地JVM Process进程中，通常开发测试使用。</p></li>
</ul>
</li>
<li><p>集群模式：Cluster Mode</p>
<ul>
<li><p>将Spark应用程序运行在集群上，比如Hadoop YARN集群，Spark自身集群Standalone及Apache Mesos集群</p></li>
<li><p>Hadoop YARN集群模式（生产环境使用）：运行在 yarn 集群之上，由 yarn 负责资源管理，Spark 负责任务调度和计算，好处：计算资源按需伸缩，集群利用率高，共享底层存储，避免数据跨集群迁移。</p></li>
<li><p>Spark Standalone集群模式（开发测试及生成环境使用）：类似Hadoop YARN架构，典型的Mater/Slaves模式，使用Zookeeper搭建高可用，避免Master是有单点故障的。</p></li>
<li><p>Apache Mesos集群模式（国内使用较少）：运行在 Mesos 资源管理器框架之上，由 Mesos 负责资源管理，Spark 负责任务调度和计算。</p></li>
</ul>
</li>
<li><p>云服务：Kubernetes模式</p>
<ul>
<li><p>Spark 2.3开始支持将Spark开发应用运行在K8S上</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="quick-start">
<h2><span class="section-number">10.4. </span>Quick Start<a class="headerlink" href="#quick-start" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Spark 2.4.x本地模式使用Spark-Shell运行Word Count程序</p>
<p>集群环境 CentOS 7.7</p>
<ul class="simple">
<li><p>node1</p></li>
</ul>
<p>数据预先上传至HDFS目录: /datas/wordount.data</p>
</div></blockquote>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span>scala&gt; val inputRDD = sc.textFile(&quot;/datas/wordcount.data&quot;)
inputRDD: org.apache.spark.rdd.RDD[String] = /datas/wordcount.data MapPartitionsRDD[1] at textFile at &lt;console&gt;:24

scala&gt; val wordsRDD = inputRDD.flatMap(line =&gt; line.split(&quot;\\s+&quot;))
wordsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:25

scala&gt; val tuplesRDD = wordsRDD.map(word =&gt; (word, 1))
tuplesRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:25

scala&gt; val wordcountsRDD = tuplesRDD.reduceByKey((tmp, item) =&gt; tmp + item)
wordcountsRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:25

scala&gt; wordcountsRDD.foreach(item =&gt; println(item))
(spark,4)
(hadoop,1)
(hive,3)
(sprk,1)
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h2><span class="section-number">10.5. </span>Spark应用组成<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>当Spark Application程序运行在集群上时，由两部分组成：<code class="docutils literal notranslate"><span class="pre">Driver</span> <span class="pre">Program</span> <span class="pre">+</span> <span class="pre">Executors</span></code>，都是JVM进程</p>
<ul class="simple">
<li><p>1、<code class="docutils literal notranslate"><span class="pre">Driver</span> <span class="pre">Program</span></code>：应用管理者</p>
<ul>
<li><p>类似于<code class="docutils literal notranslate"><span class="pre">Application</span> <span class="pre">Master</span></code>，管理整个应用中所有的Job的调度执行</p></li>
<li><p>运行JVM Process，运行程序的Main函数，必须创建SparkContext上下文对象</p></li>
<li><p>一个SparkApplication仅有一个</p></li>
</ul>
</li>
<li><p>2、<code class="docutils literal notranslate"><span class="pre">Executors</span></code>：</p>
<ul>
<li><p>相当于一个线程池，运行JVM Process，其中有很多线程，每个线程运行一个Task任务，一个Task运行需要1 Core CPU，所以可以认为Executor中线程数就等于CPU Core核数</p></li>
<li><p>一个Spark Application可以有多个，可以设置个数和资源信息</p></li>
<li><p>类似MapTask和ReduceTask</p></li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="spark-shell">
<h2><span class="section-number">10.6. </span>使用Spark Shell<a class="headerlink" href="#spark-shell" title="永久链接至标题">¶</a></h2>
<p>本地模式运行Spark框架提供交互式命令行：spark-shell，其中本地模式LocalMode含义为：启动一个JVM Process进程，执行任务Task，使用方式如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">master</span> <span class="n">local</span> <span class="o">|</span> <span class="n">local</span><span class="p">[</span><span class="o">*</span><span class="p">]</span> <span class="o">|</span> <span class="n">local</span><span class="p">[</span><span class="n">K</span><span class="p">]</span> <span class="n">建议</span> <span class="n">K</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="n">正整数</span>
</pre></div>
</div>
<p>其中K表示启动线程数目（使用CPU核心数）</p>
<ul class="simple">
<li><p>Spark中Task以Thread方式运行</p></li>
<li><p>每个Task运行需要1 Core CPU</p></li>
</ul>
<p>本地模式启动spark shell：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 进入Spark安装目录</span>
<span class="nb">cd</span> /export/server/spark
<span class="c1">## 启动spark-shell</span>
bin/spark-shell --master local<span class="o">[</span><span class="m">2</span><span class="o">]</span>
</pre></div>
</div>
</div>
<div class="section" id="spark-standalone">
<h2><span class="section-number">10.7. </span>Spark Standalone集群<a class="headerlink" href="#spark-standalone" title="永久链接至标题">¶</a></h2>
<div class="section" id="id4">
<h3><span class="section-number">10.7.1. </span>架构<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p><img alt="1605682900705" src="../_images/1605682900705.png" /></p>
<p>Spark Standalone集群，类似于Hadoop YARN，管理集群资源和调度资源</p>
<ul class="simple">
<li><p>主节点Master：类似ResourceManager</p>
<ul>
<li><p>管理整个集群资源，接收提交应用，分配资源给每个应用，运行Task任务</p></li>
</ul>
</li>
<li><p>从节点Workers：类似NodeManager</p>
<ul>
<li><p>管理每个机器的资源，分配对应的资源来运行Task；</p></li>
<li><p>每个从节点分配资源信息给Worker管理，资源信息包含内存Memory和CPU Cores核数</p></li>
</ul>
</li>
<li><p>历史服务器HistoryServer：类似MRJobHistoryServer</p>
<ul>
<li><p>Spark Application运行完成以后，保存事件日志数据至HDFS，启动HistoryServer可以查应用运行相关信息</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id5">
<h3><span class="section-number">10.7.2. </span>搭建步骤<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h3>
<div class="section" id="step1">
<h4><span class="section-number">10.7.2.1. </span>Step1：解压安装<a class="headerlink" href="#step1" title="永久链接至标题">¶</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 解压软件包</span>
tar -zxf /export/software/spark-2.4.5-bin-cdh5.16.2-2.11.tgz -C /export/server/
<span class="c1">## 创建软连接，方便后期升级</span>
ln -s /export/server/spark-2.4.5-bin-cdh5.16.2-2.11 /export/server/spark
<span class="c1">## 进入配置目录</span>
<span class="nb">cd</span> /export/server/spark/conf
<span class="c1">## 修改配置文件名称</span>
mv spark-env.sh.template spark-env.sh
vim spark-env.sh
<span class="c1">## 添加内容如下：</span>
<span class="nv">JAVA_HOME</span><span class="o">=</span>/export/server/jdk
<span class="nv">SCALA_HOME</span><span class="o">=</span>/export/server/scala
<span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>/export/server/hadoop/etc/hadoop
</pre></div>
</div>
</div>
<div class="section" id="step2-slaves">
<h4><span class="section-number">10.7.2.2. </span>Step2：修改Slaves文件<a class="headerlink" href="#step2-slaves" title="永久链接至标题">¶</a></h4>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span>## 进入配置目录
cd /export/server/spark/conf
## 修改配置文件名称
mv slaves.template slaves
vim slaves
## 内容如下：
node1
node2
node3
</pre></div>
</div>
</div>
<div class="section" id="step3-spark-env-sh">
<h4><span class="section-number">10.7.2.3. </span>Step3：修改Spark-env.sh<a class="headerlink" href="#step3-spark-env-sh" title="永久链接至标题">¶</a></h4>
<ul>
<li><p>配置Master、Workers、HistoryServer</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">SPARK_MASTER_HOST</span><span class="o">=</span><span class="s">node1</span>
<span class="na">SPARK_MASTER_PORT</span><span class="o">=</span><span class="s">7077</span>
<span class="na">SPARK_MASTER_WEBUI_PORT</span><span class="o">=</span><span class="s">8080</span>
<span class="na">SPARK_WORKER_CORES</span><span class="o">=</span><span class="s">1</span>
<span class="na">SPARK_WORKER_MEMORY</span><span class="o">=</span><span class="s">1g</span>
<span class="na">SPARK_WORKER_PORT</span><span class="o">=</span><span class="s">7078</span>
<span class="na">SPARK_WORKER_WEBUI_PORT</span><span class="o">=</span><span class="s">8081</span>
<span class="na">SPARK_HISTORY_OPTS</span><span class="o">=</span><span class="s">&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/spark/eventLogs/  -Dspark.history.fs.cleaner.enabled=true&quot;</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="step4">
<h4><span class="section-number">10.7.2.4. </span>Step4：创建日志存储目录<a class="headerlink" href="#step4" title="永久链接至标题">¶</a></h4>
<p>启动HDFS，创建应用日志存储目录</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>hadoop-daemon.sh start namenode
hadoop-daemons.sh start datanode
hdfs dfs -mkdir -p /spark/eventLogs/
</pre></div>
</div>
</div>
<div class="section" id="step5-spark-default-conf">
<h4><span class="section-number">10.7.2.5. </span>Step5：修改Spark-default.conf<a class="headerlink" href="#step5-spark-default-conf" title="永久链接至标题">¶</a></h4>
<ul>
<li><p>配置Spark应用保存EventLogs</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 进入配置目录</span>
<span class="nb">cd</span> /export/server/spark/conf
<span class="c1">## 修改配置文件名称</span>
mv spark-defaults.conf.template spark-defaults.conf
vim spark-defaults.conf
<span class="c1">## 添加内容如下：</span>
spark.eventLog.enabled <span class="nb">true</span>
spark.eventLog.dir hdfs://node1:8020/spark/eventLogs/
spark.eventLog.compress <span class="nb">true</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="step6-log4j-properties">
<h4><span class="section-number">10.7.2.6. </span>Step6：修改log4j.properties<a class="headerlink" href="#step6-log4j-properties" title="永久链接至标题">¶</a></h4>
<ul>
<li><p>设置日志级别</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span>## 进入目录
cd /export/server/spark/conf
## 修改日志属性配置文件名称
mv log4j.properties.template log4j.properties
## 改变日志级别
vim log4j.properties
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="step7">
<h4><span class="section-number">10.7.2.7. </span>Step7：分发配置至其余机器<a class="headerlink" href="#step7" title="永久链接至标题">¶</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /export/server/
scp -r spark-2.4.5-bin-cdh5.16.2-2.11 root@node2:<span class="nv">$PWD</span>
scp -r spark-2.4.5-bin-cdh5.16.2-2.11 root@ node3:<span class="nv">$PWD</span>
<span class="c1">## 远程连接到node2和node3机器，创建软连接</span>
ln -s /export/server/spark-2.4.5-bin-cdh5.16.2-2.11 /export/server/spark
</pre></div>
</div>
</div>
</div>
<div class="section" id="id6">
<h3><span class="section-number">10.7.3. </span>启动服务<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<ul>
<li><p>在主节点启动Master</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/export/server/spark/sbin/start-master.sh
</pre></div>
</div>
<ul class="simple">
<li><p>查看WEB-UI：http://node1:8080</p></li>
</ul>
</li>
<li><p>在主节点启动从节点</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/export/server/spark/sbin/start-slaves.sh
</pre></div>
</div>
<ul class="simple">
<li><p>查看WEB-UI：http://node1:8080 可以看到从节点上线加入集群</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="spark-submit">
<h2><span class="section-number">10.8. </span>提交程序运行Spark Submit<a class="headerlink" href="#spark-submit" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>可以在命令行使用<code class="docutils literal notranslate"><span class="pre">$SPARK_HOME/bin/spark-submit</span> <span class="pre">--help</span></code>来获取相关命令的帮助</p>
</div></blockquote>
<div class="section" id="id7">
<h3><span class="section-number">10.8.1. </span>示例<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">--master</span> <span class="pre">spark://node1:7077</span></code>表示Standalone地址</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">SPARK_HOME</span><span class="o">=</span>/export/server/spark

<span class="si">${</span><span class="nv">SPARK_HOME</span><span class="si">}</span>/bin/spark-submit <span class="se">\</span>
--master spark://node1:7077 <span class="se">\</span>
--class org.apache.spark.examples.SparkPi <span class="se">\</span>
<span class="si">${</span><span class="nv">SPARK_HOME</span><span class="si">}</span>/examples/jars/spark-examples_2.11-2.4.5.jar <span class="se">\</span>
<span class="m">10</span>
</pre></div>
</div>
</div>
<div class="section" id="id8">
<h3><span class="section-number">10.8.2. </span>提交参数<a class="headerlink" href="#id8" title="永久链接至标题">¶</a></h3>
<div class="section" id="id9">
<h4><span class="section-number">10.8.2.1. </span>基本参数<a class="headerlink" href="#id9" title="永久链接至标题">¶</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 表示运行的模式 本地模式local 集群模式</span>
--master MASTER_URL
<span class="c1">#本地模式local[2]     Standalone集群 spark://domain1.port, spark://domain2:port</span>

<span class="c1"># Driver Program运行的地方 也表示集群的部署模式默认为client 生产环境通常使用cluster</span>
--deploy-mode DEPLOY_MODE

<span class="c1"># 表示要运行的Application的类名称</span>
--class CLASS_NAME

<span class="c1"># 应用运行的全名 </span>
--name  A NAME OF YOUR APPLICATION

<span class="c1"># 要运行的jar包名称 通常在本地文件系统中 多个jar包用逗号隔开</span>
--jar JARS

<span class="c1"># 参数配置</span>
--conf <span class="nv">PROP</span><span class="o">=</span>VALUE
</pre></div>
</div>
</div>
<div class="section" id="driver-program">
<h4><span class="section-number">10.8.2.2. </span>Driver Program参数<a class="headerlink" href="#driver-program" title="永久链接至标题">¶</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 指定Driver Program JVM内存大小 默认为1G</span>
--driver-memory MEM

<span class="c1"># 表示Driver运行CLASS PATH路径</span>
--driver-class-path

<span class="c1"># Spark Standalone with cluster deploy mode 默认值为1</span>
--driver-cores NUM

<span class="c1"># 运行在YARN in cluster mode 默认值为1</span>
--driver-cores NUM

<span class="c1">#运行在Standalone的部署模式下，如果Driver运行异常而失败，可以自动重启</span>
--supervise
</pre></div>
</div>
</div>
</div>
<div class="section" id="executor">
<h3><span class="section-number">10.8.3. </span>Executor 参数配置<a class="headerlink" href="#executor" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>每个Spark Application运行时，需要启动Executor运行任务Task，需要指定Executor个数及每个Executor资源信息（内存Memory和CPU Core核数）</p>
</div></blockquote>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">#Executor运行所需内存大小</span>
 --executor-memory MEM
 
<span class="c1">#Execturo运行的CPU Cores,默认的情况下，在Standalone集群上为worker节点所有可有的Cpu Cores,在YARN集群下为1</span>
 --executor-cores NUM
 
<span class="c1">#表示运行在Standalone集群下，所有Executor的CPU Cores,结合--executor-cores计算出Executor个数</span>
--total-executor-cores NUM

<span class="c1">#表示在YARN集群下，Executor的个数，默认值为2</span>
 --num-executors
 
<span class="c1">#表示Executor运行的队列，默认为default队列</span>
--queue QUEUE_NAME
</pre></div>
</div>
</div>
<div class="section" id="id10">
<h3><span class="section-number">10.8.4. </span>官方案例<a class="headerlink" href="#id10" title="永久链接至标题">¶</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run application locally on 8 cores</span>
./bin/spark-submit <span class="se">\</span>
 --class org.apache.spark.examples.SparkPi <span class="se">\</span>
 --master local<span class="o">[</span><span class="m">8</span><span class="o">]</span> <span class="se">\</span>
 /path/to/examples.jar <span class="se">\</span>
 <span class="m">100</span>
 
<span class="c1"># Run on a Spark standalone cluster in client deploy mode</span>
./bin/spark-submit <span class="se">\</span>
 --class org.apache.spark.examples.SparkPi <span class="se">\</span>
 --master spark://207.184.161.138:7077 <span class="se">\</span>
 --executor-memory 20G <span class="se">\</span>
 --total-executor-cores <span class="m">100</span> <span class="se">\</span>
 /path/to/examples.jar <span class="se">\</span>
 <span class="m">1000</span>
 
<span class="c1"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span>
./bin/spark-submit <span class="se">\</span>
 --class org.apache.spark.examples.SparkPi <span class="se">\</span>
 --master spark://207.184.161.138:7077 <span class="se">\</span>
 --deploy-mode cluster <span class="se">\</span>
 --supervise <span class="se">\</span>
 --executor-memory 20G <span class="se">\</span>
 --total-executor-cores <span class="m">100</span> <span class="se">\</span>
 /path/to/examples.jar <span class="se">\</span>
 <span class="m">1000</span>
 
<span class="c1"># Run on a YARN cluster</span>
<span class="nb">export</span> <span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>XXX
./bin/spark-submit <span class="se">\</span>
 --class org.apache.spark.examples.SparkPi <span class="se">\</span>
 --master yarn <span class="se">\</span>
 --deploy-mode cluster <span class="se">\ </span><span class="c1"># can be client for client mode</span>
 --executor-memory 20G <span class="se">\</span>
 --num-executors <span class="m">50</span> <span class="se">\</span>
 /path/to/examples.jar <span class="se">\</span>
 <span class="m">1000</span>
 
<span class="c1"># Run a Python application on a Spark standalone cluster</span>
./bin/spark-submit <span class="se">\</span>
 --master spark://207.184.161.138:7077 <span class="se">\</span>
 examples/src/main/python/pi.py <span class="se">\</span>
 <span class="m">1000</span>
 
<span class="c1"># Run on a Mesos cluster in cluster deploy mode with supervise</span>
./bin/spark-submit <span class="se">\</span>
 --class org.apache.spark.examples.SparkPi <span class="se">\</span>
--master mesos://207.184.161.138:7077 <span class="se">\</span>
 --deploy-mode cluster <span class="se">\</span>
 --supervise <span class="se">\</span>
 --executor-memory 20G <span class="se">\</span>
 --total-executor-cores <span class="m">100</span> <span class="se">\</span>
 http://path/to/examples.jar <span class="se">\</span>
 <span class="m">1000</span>
 
<span class="c1"># Run on a Kubernetes cluster in cluster deploy mode</span>
./bin/spark-submit <span class="se">\</span>
 --class org.apache.spark.examples.SparkPi <span class="se">\</span>
 --master k8s://xx.yy.zz.ww:443 <span class="se">\</span>
 --deploy-mode cluster <span class="se">\</span>
 --executor-memory 20G <span class="se">\</span>
 --num-executors <span class="m">50</span> <span class="se">\</span>
 http://path/to/examples.jar <span class="se">\</span>
 <span class="m">1000</span>
</pre></div>
</div>
<div class="section" id="web-ui">
<h4><span class="section-number">10.8.4.1. </span>WEB-UI监控<a class="headerlink" href="#web-ui" title="永久链接至标题">¶</a></h4>
<ul>
<li><p>Spark 提供了多个监控界面，当运行Spark任务后可以直接在网页对各种信息进行监控查看。运行spark-shell交互式命令在Standalone集群上，命令如下：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/export/server/spark/bin/spark-shell --master spark://node1:7077
</pre></div>
</div>
</li>
<li><p>Spark Application程序运行时三个核心概念：Job、Stage、Task</p>
<ul class="simple">
<li><p>Task：被分配到各个 Executor 的单位工作内容，它是 Spark 中的最小执行单位，一
般来说有多少个 Paritition（物理层面的概念，即分支可以理解为将数据划分成不同
部分并行处理），就会有多少个 Task，每个 Task 只会处理单一分支上的数据。</p></li>
<li><p>Job：由多个 Task 的并行计算部分，一般 Spark 中的 action 操作（如 save、collect，后面
进一步说明），会生成一个 Job。</p></li>
<li><p>Stage：Job 的组成单位，一个 Job 会切分成多个 Stage，Stage 彼此之间相互依赖顺序执行，
而每个 Stage 是多个 Task 的集合，类似 map 和 reduce stage。</p></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="spark-standalone-ha">
<h2><span class="section-number">10.9. </span>Spark Standalone  HA<a class="headerlink" href="#spark-standalone-ha" title="永久链接至标题">¶</a></h2>
<div class="section" id="id11">
<h3><span class="section-number">10.9.1. </span>搭建配置<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
<ul>
<li><p>停止集群</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 在node1上执行命令</span>
/export/server/spark/sbin/stop-master.sh
/export/server/spark/sbin/stop-slaves.sh
</pre></div>
</div>
</li>
<li><p>增加Zookeeper配置</p>
<ul>
<li><p>对Spark配置文件【$SPARK_HOME/conf/spark-env.sh】文件如下修改</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="na">SPARK_DAEMON_JAVA_OPTS</span><span class="o">=</span><span class="s">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER </span>
<span class="na">-Dspark.deploy.zookeeper.url</span><span class="o">=</span><span class="s">node1:2181,node2:2181,node3:2181 </span>
<span class="na">-Dspark.deploy.zookeeper.dir</span><span class="o">=</span><span class="s">/spark-ha&quot;</span>
</pre></div>
</div>
</li>
<li><p>说明</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>spark.deploy.recoveryMode：恢复模式
spark.deploy.zookeeper.url：ZooKeeper的Server地址
spark.deploy.zookeeper.dir：保存集群元数据信息的文件、目录。包括Worker、Driver、Application信息。
</pre></div>
</div>
</li>
<li><p>注释或删除MASTER_HOST内容：</p>
<div class="highlight-properties notranslate"><div class="highlight"><pre><span></span><span class="c"># SPARK_MASTER_HOST=node1</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>将spark-env.sh分发集群</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /export/server/spark/conf
scp -r spark-env.sh root@node2:<span class="nv">$PWD</span>
scp -r spark-env.sh root@node3:<span class="nv">$PWD</span>
</pre></div>
</div>
</li>
<li><p>启动集群服务</p>
<ul>
<li><p>先启动Zookeeper集群，再分别启动2个Master服务，最后启动Worker服务</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 启动ZOOKEEPER服务</span>
zkServer.sh start
<span class="c1">## 在node1和node2分别启动Master服务</span>
/export/server/spark/sbin/start-master.sh
<span class="c1">## 查看哪个Master为Active，就在哪个Master机器上启动Workers服务</span>
/export/server/spark/sbin/start-slaves.sh
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id12">
<h3><span class="section-number">10.9.2. </span>测试运行<a class="headerlink" href="#id12" title="永久链接至标题">¶</a></h3>
<p>Standalone HA集群运行应用时，指定ClusterManager参数属性为</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">--master spark://host1:port1,host2:port2</span>
</pre></div>
</div>
<p>提交圆周率PI运行集群，命令如下：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">SPARK_HOME</span><span class="o">=</span>/export/server/spark
<span class="si">${</span><span class="nv">SPARK_HOME</span><span class="si">}</span>/bin/spark-submit <span class="se">\</span>
--master spark://node1.itcast.cn:7077,node2.itcast.cn:7077 <span class="se">\</span>
--class org.apache.spark.examples.SparkPi <span class="se">\</span>
<span class="si">${</span><span class="nv">SPARK_HOME</span><span class="si">}</span>/examples/jars/spark-examples_2.11-2.4.5.jar <span class="se">\</span>
<span class="m">100</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="spark-on-yarn">
<h2><span class="section-number">10.10. </span>Spark On Yarn<a class="headerlink" href="#spark-on-yarn" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>无论是MapReduce、Flink、Spark应用程序，往往运行在Yarn上</p>
<ul class="simple">
<li><p>统一资源管理，节约运维成本</p></li>
<li><p>充分使用集群</p></li>
</ul>
</div></blockquote>
<div class="section" id="id13">
<h3><span class="section-number">10.10.1. </span>搭建步骤<a class="headerlink" href="#id13" title="永久链接至标题">¶</a></h3>
<div class="section" id="spark-env-sh">
<h4><span class="section-number">10.10.1.1. </span>修改spark-env.sh文件<a class="headerlink" href="#spark-env-sh" title="永久链接至标题">¶</a></h4>
<div class="section" id="id14">
<h5><span class="section-number">10.10.1.1.1. </span>添加<a class="headerlink" href="#id14" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>vim /export/server/spark/conf/spark-env.sh
<span class="c1">## 添加内容</span>
<span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span>/export/server/hadoop/etc/hadoop
<span class="nv">YARN_CONF_DIR</span><span class="o">=</span>/export/server/hadoop/etc/hadoop
</pre></div>
</div>
</div>
<div class="section" id="id15">
<h5><span class="section-number">10.10.1.1.2. </span>分发同步<a class="headerlink" href="#id15" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /export/server/spark/conf
scp -r spark-env.sh root@node2:<span class="nv">$PWD</span>
scp -r spark-env.sh root@node3:<span class="nv">$PWD</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="yarn-site-xml">
<h4><span class="section-number">10.10.1.2. </span>修改Yarn-site.xml文件<a class="headerlink" href="#yarn-site-xml" title="永久链接至标题">¶</a></h4>
<div class="section" id="id16">
<h5><span class="section-number">10.10.1.2.1. </span>添加<a class="headerlink" href="#id16" title="永久链接至标题">¶</a></h5>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span>## 在node1上修改
vim /export/server/hadoop/etc/hadoop/yarn-site.xml
## 添加内容
<span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>yarn.log-aggregation-enable<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>yarn.log-aggregation.retain-seconds<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>604800<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>yarn.log.server.url<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>http://node1:19888/jobhistory/logs<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="id17">
<h5><span class="section-number">10.10.1.2.2. </span>分发同步<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /export/server/hadoop/etc/hadoop
scp -r yarn-site.xml root@node2:<span class="nv">$PWD</span>
scp -r yarn-site.xml root@node3:<span class="nv">$PWD</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="spark-default-conf">
<h4><span class="section-number">10.10.1.3. </span>修改spark-default.conf<a class="headerlink" href="#spark-default-conf" title="永久链接至标题">¶</a></h4>
<div class="section" id="id18">
<h5><span class="section-number">10.10.1.3.1. </span>添加<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 在node1上修改</span>
vim /export/server/spark/conf/spark-defaults.conf
<span class="c1">## 添加内容</span>
spark.yarn.historyServer.address node1:18080
</pre></div>
</div>
</div>
<div class="section" id="id19">
<h5><span class="section-number">10.10.1.3.2. </span>分发同步<a class="headerlink" href="#id19" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /export/server/spark/conf
scp -r spark-defaults.conf root@node2:<span class="nv">$PWD</span>
scp -r spark-defaults.conf root@node3:<span class="nv">$PWD</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="jars">
<h4><span class="section-number">10.10.1.4. </span>配置依赖JARS<a class="headerlink" href="#jars" title="永久链接至标题">¶</a></h4>
<blockquote>
<div><p>当Spark Application应用提交运行在YARN上时，默认情况下，每次提交应用都需要将依赖
Spark相关jar包上传到YARN 集群中，为了节省提交时间和存储空间，将Spark相关jar包上传到
HDFS目录中，设置属性告知Spark Application应用。</p>
</div></blockquote>
<div class="section" id="id20">
<h5><span class="section-number">10.10.1.4.1. </span>添加<a class="headerlink" href="#id20" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 启动HDFS，在node1上操作</span>
hadoop-daemon.sh start namenode
hadoop-daemons.sh start datanode
<span class="c1">## hdfs上创建存储spark相关jar包目录</span>
hdfs dfs -mkdir -p /spark/apps/jars/
<span class="c1">## 上传$SPARK_HOME/jars所有jar包</span>
hdfs dfs -put /export/server/spark/jars/* /spark/apps/jars/
</pre></div>
</div>
<ul>
<li><p>spark-defatult.conf文件增加Spark相关JAR包存储的HDFS目录</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 在node1上操作</span>
vim /export/server/spark/conf/spark-defaults.conf
<span class="c1">## 添加内容</span>
spark.yarn.jars hdfs://node1:8020/spark/apps/jars/*
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="id21">
<h5><span class="section-number">10.10.1.4.2. </span>分发同步<a class="headerlink" href="#id21" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 在node1上操作</span>
<span class="nb">cd</span> /export/server/spark/conf
scp -r spark-defaults.conf root@node2:<span class="nv">$PWD</span>
scp -r spark-defaults.conf root@node3:<span class="nv">$PWD</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="yarn">
<h4><span class="section-number">10.10.1.5. </span>Yarn资源检查<a class="headerlink" href="#yarn" title="永久链接至标题">¶</a></h4>
<div class="section" id="id22">
<h5><span class="section-number">10.10.1.5.1. </span>设置资源不检查<a class="headerlink" href="#id22" title="永久链接至标题">¶</a></h5>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span>## 编辑yarn-site.xml文件，在node1上操作
vim /export/server/hadoop/etc/hadoop/yarn-site.xml
## 添加内容
<span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
 <span class="nt">&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="nt">&lt;/name&gt;</span>
 <span class="nt">&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="id23">
<h5><span class="section-number">10.10.1.5.2. </span>分发同步<a class="headerlink" href="#id23" title="永久链接至标题">¶</a></h5>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> /export/server/hadoop/etc/hadoop
scp -r yarn-site.xml root@node2:<span class="nv">$PWD</span>
scp -r yarn-site.xml root@node3:<span class="nv">$PWD</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id24">
<h3><span class="section-number">10.10.2. </span>启动服务<a class="headerlink" href="#id24" title="永久链接至标题">¶</a></h3>
<p>启动HDFS、YARN、MRHistoryServer和Spark HistoryServer</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1">## 启动HDFS和YARN服务，在node1执行命令</span>
hadoop-daemon.sh start namenode
hadoop-daemons.sh start datanode
yarn-daemon.sh start resourcemanager
yarn-daemons.sh start nodemanager
<span class="c1">## 启动MRHistoryServer服务，在node1执行命令</span>
mr-jobhistory-daemon.sh start historyserver
<span class="c1">## 启动Spark HistoryServer服务，，在node1执行命令</span>
/export/server/spark/sbin/start-history-server.sh
</pre></div>
</div>
</div>
</div>
<div class="section" id="deploy-mode">
<h2><span class="section-number">10.11. </span>Deploy Mode<a class="headerlink" href="#deploy-mode" title="永久链接至标题">¶</a></h2>
<blockquote>
<div><p>Client模式和Cluster模式两种模式</p>
<p>本质的区别是：Driver Program运行在哪里</p>
<ul class="simple">
<li><p>Client模式Driver Program运行在客户端</p></li>
<li><p>Cluster模式Driver Program运行在一台Worker节点上</p></li>
</ul>
</div></blockquote>
<div class="section" id="client">
<h3><span class="section-number">10.11.1. </span>Client模式<a class="headerlink" href="#client" title="永久链接至标题">¶</a></h3>
<p><img alt="image-20201119104819164" src="../_images/image-20201119104819164.png" /></p>
<p><img alt="image-20201123223436451" src="../_images/image-20201123223436451.png" /></p>
<ul>
<li><p>具体流程</p>
<ul class="simple">
<li><p>Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster</p></li>
<li><p>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存</p></li>
<li><p>ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程</p></li>
<li><p>Executor进程启动后会向Driver反向注册，Executor全部注册完成后，Driver开始执行main函数</p></li>
<li><p>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。</p></li>
</ul>
</li>
<li><p>以运行词频统计WordCount程序为例，提交命令如下</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/export/server/spark/bin/spark-submit <span class="se">\</span>
--master yarn <span class="se">\</span>
--deploy-mode client <span class="se">\ </span>
--driver-memory 512m <span class="se">\ </span>
--executor-memory 512m <span class="se">\ </span>
--executor-cores <span class="m">1</span> <span class="se">\</span>
--num-executors <span class="m">2</span> <span class="se">\ </span>
--queue default <span class="se">\ </span>
--class me.iroohom.spark.submit.SparkSubmit <span class="se">\ </span>
hdfs://node1:8020/spark/apps/spark-chapter01_2.11-1.0.0.jar <span class="se">\</span>
/datas/wordcount.data /datas/swcy-client
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="cluster">
<h3><span class="section-number">10.11.2. </span><strong>Cluster模式</strong><a class="headerlink" href="#cluster" title="永久链接至标题">¶</a></h3>
<blockquote>
<div><p>生产环境使用的模式</p>
</div></blockquote>
<p><img alt="image-20201119104833479" src="../_images/image-20201119104833479.png" /></p>
<p><img alt="image-20201123223713066" src="../_images/image-20201123223713066.png" /></p>
<ul>
<li><p>具体流程</p>
<ul class="simple">
<li><p>任务提交后会和ResourceManager通讯申请启动ApplicationMaster</p></li>
<li><p>随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver</p></li>
<li><p>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container,然后在合适的NodeManager上启动Executor进程</p></li>
<li><p>Executor进程启动后会向Driver反向注册</p></li>
<li><p>Executor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行</p></li>
</ul>
</li>
<li><p>以运行词频统计WordCount程序为例，提交命令如下：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/export/server/spark/bin/spark-submit <span class="se">\ </span>
--master yarn <span class="se">\</span>
--deploy-mode cluster <span class="se">\</span>
--driver-memory 512m <span class="se">\ </span>
--executor-memory 512m <span class="se">\ </span>
--executor-cores <span class="m">1</span> <span class="se">\ </span>
--num-executors <span class="m">2</span> <span class="se">\ </span>
--queue default <span class="se">\ </span>
--class me.iroohom.spark.submit.SparkSubmit <span class="se">\ </span>
hdfs://node1:8020/spark/apps/spark-chapter01_2.11-1.0.0.jar <span class="se">\ </span>
/datas/wordcount.data /datas/swcy-cluster
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>对比：</p>
<ul class="simple">
<li><p>cluster模式：生产环境使用</p></li>
<li><p>Driver程序运行在YARN集群中Worker节点上</p></li>
<li><p>应用程序的结果不在客户端显示</p></li>
<li><p>client模式：开发测试使用</p></li>
<li><p>Driver程序运行在Client的SparkSubmit进程中</p></li>
<li><p>应用程序的结果会在客户端显示</p></li>
</ul>
</div></blockquote>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="SparkCore.html" class="btn btn-neutral float-right" title="11. Spark Core" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Redis.html" class="btn btn-neutral float-left" title="9. Redis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; 版权所有 2020, roohom

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>